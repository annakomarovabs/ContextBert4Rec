{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a614495f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "\n",
    "from RandomSplit import RandomSplit\n",
    "from metrics import ndcg_metric, dcg, recall_metric, evaluate_recommender, get_metrics\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import catalyst \n",
    "import recbole\n",
    "\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.init import constant_, xavier_normal_\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from catalyst import dl, metrics\n",
    "from catalyst.contrib.datasets import MovieLens\n",
    "from catalyst.utils import get_device, set_global_seed\n",
    "from torch.nn.utils.rnn import pad_sequence \n",
    "\n",
    "import random\n",
    "\n",
    "from recbole.model.abstract_recommender import SequentialRecommender\n",
    "from recbole.model.layers import *\n",
    "\n",
    "from BERT4Rec import BERT4Rec\n",
    "from runner import RecSysRunner\n",
    "from ContextBERT4Rec import ContextBERT4Rec\n",
    "\n",
    "set_global_seed(100)\n",
    "device = torch.device(\"mps\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2c046e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>weekday</th>\n",
       "      <th>hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>2000-12-31 22:12:40</td>\n",
       "      <td>6</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "      <td>2000-12-31 22:35:09</td>\n",
       "      <td>6</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "      <td>2000-12-31 22:32:48</td>\n",
       "      <td>6</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "      <td>2000-12-31 22:04:35</td>\n",
       "      <td>6</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "      <td>2001-01-06 23:38:11</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating           timestamp  weekday  hour\n",
       "0        1     1193       5 2000-12-31 22:12:40        6    22\n",
       "1        1      661       3 2000-12-31 22:35:09        6    22\n",
       "2        1      914       3 2000-12-31 22:32:48        6    22\n",
       "3        1     3408       4 2000-12-31 22:04:35        6    22\n",
       "4        1     2355       5 2001-01-06 23:38:11        5    23"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnames = ['user_id', 'movie_id', 'rating', 'timestamp']\n",
    "df = pd.read_table('data/ratings.dat', sep='::',header=None, names=rnames, engine='python')\n",
    "df = df.rename(columns={'userId': 'user_id', 'movie_id': 'item_id'})\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'],unit='s')\n",
    "df['weekday'] = pd.to_datetime(df.timestamp).dt.weekday\n",
    "df['hour'] = pd.to_datetime(df.timestamp).dt.hour\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30d6082b-7373-4e49-815e-33d0cccb5232",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RandomSplit(test_fraction=0.2)\n",
    "train_df, valid_df, test_df = splitter(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfc9fa70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>train_interactions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[(3186, 2000-12-31 22:00:19, 6, 22), (1270, 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[(1198, 2000-12-31 21:28:44, 6, 21), (1210, 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[(593, 2000-12-31 21:10:18, 6, 21), (2858, 200...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[(1210, 2000-12-31 20:18:44, 6, 20), (1097, 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>[(2717, 2000-12-31 05:37:52, 6, 5), (908, 2000...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                                 train_interactions\n",
       "0        1  [(3186, 2000-12-31 22:00:19, 6, 22), (1270, 20...\n",
       "1        2  [(1198, 2000-12-31 21:28:44, 6, 21), (1210, 20...\n",
       "2        3  [(593, 2000-12-31 21:10:18, 6, 21), (2858, 200...\n",
       "3        4  [(1210, 2000-12-31 20:18:44, 6, 20), (1097, 20...\n",
       "4        5  [(2717, 2000-12-31 05:37:52, 6, 5), (908, 2000..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_grouped = train_df.groupby('user_id').apply(\n",
    "    lambda x: [(t1, t2, t3, t4) for t1, t2, t3, t4 in sorted(zip(x.item_id, \n",
    "                                                                 x.timestamp,\n",
    "                                                                 x.weekday,\n",
    "                                                                 x.hour), key=lambda x: x[1])]\n",
    ").reset_index()\n",
    "train_grouped.rename({0:'train_interactions'}, axis=1, inplace=True)\n",
    "\n",
    "valid_grouped = valid_df.groupby('user_id').apply(\n",
    "    lambda x: [(t1, t2, t3, t4) for t1, t2, t3, t4 in sorted(zip(x.item_id,\n",
    "                                                         x.timestamp,\n",
    "                                                         x.weekday,\n",
    "                                                         x.hour), key=lambda x: x[1])]\n",
    ").reset_index()\n",
    "valid_grouped.rename({0:'valid_interactions'}, axis=1, inplace=True)\n",
    "\n",
    "test_grouped = test_df.groupby('user_id').apply(\n",
    "    lambda x: [(t1, t2, t3, t4) for t1, t2, t3, t4 in sorted(zip(x.item_id,\n",
    "                                                         x.timestamp,\n",
    "                                                         x.weekday,\n",
    "                                                         x.hour), key=lambda x: x[1])]\n",
    ").reset_index()\n",
    "test_grouped.rename({0:'test_interactions'}, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "train_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df2d2845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>train_interactions</th>\n",
       "      <th>valid_interactions</th>\n",
       "      <th>test_interactions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[(3186, 2000-12-31 22:00:19, 6, 22), (1270, 20...</td>\n",
       "      <td>[(2791, 2000-12-31 22:36:28, 6, 22), (2321, 20...</td>\n",
       "      <td>[(2687, 2001-01-06 23:37:48, 5, 23), (745, 200...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[(1198, 2000-12-31 21:28:44, 6, 21), (1210, 20...</td>\n",
       "      <td>[(2028, 2000-12-31 21:56:13, 6, 21), (2571, 20...</td>\n",
       "      <td>[(1372, 2000-12-31 21:59:01, 6, 21), (1552, 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[(593, 2000-12-31 21:10:18, 6, 21), (2858, 200...</td>\n",
       "      <td>[(648, 2000-12-31 21:24:27, 6, 21), (2735, 200...</td>\n",
       "      <td>[(1270, 2000-12-31 21:30:31, 6, 21), (1079, 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[(1210, 2000-12-31 20:18:44, 6, 20), (1097, 20...</td>\n",
       "      <td>[(2947, 2000-12-31 20:23:50, 6, 20), (1214, 20...</td>\n",
       "      <td>[(1240, 2000-12-31 20:24:20, 6, 20), (2951, 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>[(2717, 2000-12-31 05:37:52, 6, 5), (908, 2000...</td>\n",
       "      <td>[(2323, 2000-12-31 06:50:45, 6, 6), (272, 2000...</td>\n",
       "      <td>[(1715, 2000-12-31 06:58:11, 6, 6), (1653, 200...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                                 train_interactions  \\\n",
       "0        1  [(3186, 2000-12-31 22:00:19, 6, 22), (1270, 20...   \n",
       "1        2  [(1198, 2000-12-31 21:28:44, 6, 21), (1210, 20...   \n",
       "2        3  [(593, 2000-12-31 21:10:18, 6, 21), (2858, 200...   \n",
       "3        4  [(1210, 2000-12-31 20:18:44, 6, 20), (1097, 20...   \n",
       "4        5  [(2717, 2000-12-31 05:37:52, 6, 5), (908, 2000...   \n",
       "\n",
       "                                  valid_interactions  \\\n",
       "0  [(2791, 2000-12-31 22:36:28, 6, 22), (2321, 20...   \n",
       "1  [(2028, 2000-12-31 21:56:13, 6, 21), (2571, 20...   \n",
       "2  [(648, 2000-12-31 21:24:27, 6, 21), (2735, 200...   \n",
       "3  [(2947, 2000-12-31 20:23:50, 6, 20), (1214, 20...   \n",
       "4  [(2323, 2000-12-31 06:50:45, 6, 6), (272, 2000...   \n",
       "\n",
       "                                   test_interactions  \n",
       "0  [(2687, 2001-01-06 23:37:48, 5, 23), (745, 200...  \n",
       "1  [(1372, 2000-12-31 21:59:01, 6, 21), (1552, 20...  \n",
       "2  [(1270, 2000-12-31 21:30:31, 6, 21), (1079, 20...  \n",
       "3  [(1240, 2000-12-31 20:24:20, 6, 20), (2951, 20...  \n",
       "4  [(1715, 2000-12-31 06:58:11, 6, 6), (1653, 200...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined = train_grouped.merge(valid_grouped).merge(test_grouped)\n",
    "joined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2aafba8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6040it [00:00, 40585.64it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3636"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_items = set()\n",
    "for idx, row in tqdm(joined.iterrows()):\n",
    "    for el in row.train_interactions:\n",
    "        our_items.add(el[0])\n",
    "        \n",
    "len(our_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83f4973a",
   "metadata": {},
   "outputs": [],
   "source": [
    "item2idx = {k: i for i, k in enumerate(our_items)}\n",
    "idx2item = {i: k for k, i in item2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cdd1e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6040 6040\n"
     ]
    }
   ],
   "source": [
    "class MyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, ds, num_items, item2idx, phase='valid', N=200):\n",
    "        super().__init__()\n",
    "        self.ds = ds\n",
    "        self.phase = phase\n",
    "        self.n_items = num_items\n",
    "        self.item2idx = item2idx\n",
    "        self.N = N \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        row = self.ds.iloc[idx]\n",
    "        \n",
    "        x_input = np.zeros(self.n_items+1)\n",
    "        x_input[[self.item2idx[x[0]]+1 for x in row['train_interactions'] if x[0] in self.item2idx]] = 1\n",
    "        \n",
    "        days_of_weeks = [x[2] for x in row['train_interactions'] if x[0] in self.item2idx][-self.N+1:]\n",
    "        \n",
    "        hours = [x[3] for x in row['train_interactions'] if x[0] in self.item2idx][-self.N+1:]\n",
    "        \n",
    "        seq_input = [self.item2idx[x[0]]+1 for x in row['train_interactions'] if x[0] in self.item2idx][-self.N+1:]\n",
    "        \n",
    "        targets = np.zeros(self.n_items+1)\n",
    "        \n",
    "        dow_valid = row['valid_interactions'][0][2]\n",
    "        dow_test = row['test_interactions'][0][2]\n",
    "        \n",
    "        hours_valid = row['valid_interactions'][0][3]\n",
    "        hours_test = row['test_interactions'][0][3]\n",
    "        \n",
    "        if self.phase == 'train':\n",
    "            return (seq_input, days_of_weeks, hours, dow_valid, hours_valid)\n",
    "        elif self.phase == 'valid':\n",
    "            targets[[self.item2idx[x[0]]+1 for x in row['valid_interactions'] if x[0] in self.item2idx]] = 1\n",
    "        else:\n",
    "            return (seq_input, days_of_weeks, hours, dow_test, hours_test)\n",
    "            \n",
    "        return (targets, seq_input, days_of_weeks, hours, dow_valid, hours_valid)\n",
    "     \n",
    "n_items = len(item2idx)\n",
    "\n",
    "train = MyDataset(ds=joined,\n",
    "                  num_items=n_items, \n",
    "                  item2idx=item2idx,\n",
    "                  phase='train')\n",
    "\n",
    "valid = MyDataset(ds=joined,\n",
    "                  num_items=n_items,\n",
    "                  item2idx=item2idx,\n",
    "                  phase='valid')\n",
    "\n",
    "print(len(train),len(valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f691fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_train(batch: List[Tuple[torch.Tensor]]) -> Dict[str, torch.Tensor]: \n",
    "    \n",
    "    seq_i,days_of_weeks,hours,dow_valid,hours_valid = zip(*batch)\n",
    "    seq_len = torch.Tensor([len(x) for x in seq_i])\n",
    "    dow_valid = torch.Tensor([x for x in dow_valid])\n",
    "    hours_valid = torch.Tensor([x for x in hours_valid])\n",
    "    seq_i = pad_sequence([torch.Tensor(t) for t in seq_i]).T    \n",
    "    days_of_weeks = pad_sequence([torch.Tensor(t) for t in days_of_weeks]).T\n",
    "    hours = pad_sequence([torch.Tensor(t) for t in hours]).T\n",
    "    \n",
    "    return {'seq_i': seq_i, \n",
    "            'seq_len':seq_len,\n",
    "            'dow': days_of_weeks,\n",
    "            'hours': hours,\n",
    "            'dow_valid': dow_valid,\n",
    "            'hours_valid': hours_valid}\n",
    "\n",
    "\n",
    "def collate_fn_valid(batch: List[Tuple[torch.Tensor]]) -> Dict[str, torch.Tensor]:\n",
    "    \n",
    "    y, seq_i, days_of_weeks, hours, dow_valid, hours_valid = zip(*batch)\n",
    "    \n",
    "    seq_len = torch.Tensor([len(x) for x in seq_i]).long()\n",
    "    seq_i = pad_sequence([torch.Tensor(t) for t in seq_i]).T.long()\n",
    "    days_of_weeks = pad_sequence([torch.Tensor(t) for t in days_of_weeks]).T.long()\n",
    "    hours = pad_sequence([torch.Tensor(t) for t in hours]).T.long()\n",
    "    dow_valid = torch.Tensor([x for x in dow_valid])\n",
    "    hours_valid = torch.Tensor([x for x in hours_valid])\n",
    "            \n",
    "    targets = pad_sequence([torch.Tensor(t) for t in y]).T\n",
    "\n",
    "    return {\"targets\": targets,\n",
    "            'seq_i': seq_i,\n",
    "            'seq_len':seq_len,\n",
    "            'dow': days_of_weeks,\n",
    "            'hours': hours,\n",
    "            'dow_valid': dow_valid,\n",
    "            'hours_valid': hours_valid}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ab42228",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = {\n",
    "        \"train\": DataLoader(train, batch_size=256, collate_fn=collate_fn_train),\n",
    "        \"valid\": DataLoader(valid, batch_size=256, collate_fn=collate_fn_valid),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ab9f442-bb02-4cca-bee4-369c2f1a4130",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head Self-attention layers, a attention score dropout layer is introduced.\n",
    "    Args:\n",
    "        input_tensor (torch.Tensor): the input of the multi-head self-attention layer\n",
    "        attention_mask (torch.Tensor): the attention mask for input tensor\n",
    "    Returns:\n",
    "        hidden_states (torch.Tensor): the output of the multi-head self-attention layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_heads,\n",
    "        hidden_size,\n",
    "        hidden_dropout_prob,\n",
    "        attn_dropout_prob,\n",
    "        layer_norm_eps,\n",
    "    ):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        if hidden_size % n_heads != 0:\n",
    "            raise ValueError(\n",
    "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
    "                \"heads (%d)\" % (hidden_size, n_heads)\n",
    "            )\n",
    "\n",
    "        self.num_attention_heads = n_heads\n",
    "        self.attention_head_size = int(hidden_size / n_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "        self.sqrt_attention_head_size = math.sqrt(self.attention_head_size)\n",
    "\n",
    "        self.query = nn.Linear(hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(hidden_size, self.all_head_size)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.attn_dropout = nn.Dropout(attn_dropout_prob)\n",
    "\n",
    "        self.dense = nn.Linear(hidden_size, hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(hidden_size, eps=layer_norm_eps)\n",
    "        self.out_dropout = nn.Dropout(hidden_dropout_prob)\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (\n",
    "            self.num_attention_heads,\n",
    "            self.attention_head_size,\n",
    "        )\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x\n",
    "\n",
    "    def forward(self, input_tensor, attention_mask, return_explanations=False):\n",
    "        mixed_query_layer = self.query(input_tensor)\n",
    "        mixed_key_layer = self.key(input_tensor)\n",
    "        mixed_value_layer = self.value(input_tensor)\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer).permute(0, 2, 1, 3)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer).permute(0, 2, 3, 1)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer).permute(0, 2, 1, 3)\n",
    "\n",
    "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
    "        attention_scores = torch.matmul(query_layer, key_layer)\n",
    "\n",
    "        attention_scores = attention_scores / self.sqrt_attention_head_size\n",
    "        # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n",
    "        # [batch_size heads seq_len seq_len] scores\n",
    "        # [batch_size 1 1 seq_len]\n",
    "        attention_scores = attention_scores + attention_mask\n",
    "\n",
    "        # Normalize the attention scores to probabilities.\n",
    "        attention_probs = self.softmax(attention_scores)\n",
    "        # This is actually dropping out entire tokens to attend to, which might\n",
    "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
    "\n",
    "        attention_probs = self.attn_dropout(attention_probs)\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "        hidden_states = self.dense(context_layer)\n",
    "        hidden_states = self.out_dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        \n",
    "        if return_explanations:\n",
    "            return hidden_states, attention_probs\n",
    "        else:\n",
    "            return hidden_states\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "class TransformerLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    One transformer layer consists of a multi-head self-attention layer and a point-wise feed-forward layer.\n",
    "    Args:\n",
    "        hidden_states (torch.Tensor): the input of the multi-head self-attention sublayer\n",
    "        attention_mask (torch.Tensor): the attention mask for the multi-head self-attention sublayer\n",
    "    Returns:\n",
    "        feedforward_output (torch.Tensor): The output of the point-wise feed-forward sublayer,\n",
    "                                           is the output of the transformer layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_heads,\n",
    "        hidden_size,\n",
    "        intermediate_size,\n",
    "        hidden_dropout_prob,\n",
    "        attn_dropout_prob,\n",
    "        hidden_act,\n",
    "        layer_norm_eps,\n",
    "    ):\n",
    "        super(TransformerLayer, self).__init__()\n",
    "        self.multi_head_attention = MultiHeadAttention(\n",
    "            n_heads, hidden_size, hidden_dropout_prob, attn_dropout_prob, layer_norm_eps\n",
    "        )\n",
    "        self.feed_forward = FeedForward(\n",
    "            hidden_size,\n",
    "            intermediate_size,\n",
    "            hidden_dropout_prob,\n",
    "            hidden_act,\n",
    "            layer_norm_eps,\n",
    "        )\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask,return_explanations=False):\n",
    "        \n",
    "        if return_explanations:\n",
    "            attention_output, expl = self.multi_head_attention(hidden_states, attention_mask,\n",
    "                                                         return_explanations=return_explanations)\n",
    "            \n",
    "        else:\n",
    "            attention_output = self.multi_head_attention(hidden_states, attention_mask,\n",
    "                                                         return_explanations=return_explanations)\n",
    "        feedforward_output = self.feed_forward(attention_output)\n",
    "        \n",
    "        if return_explanations:\n",
    "            return feedforward_output, expl\n",
    "        else:\n",
    "            return feedforward_output\n",
    "    \n",
    "    \n",
    "    \n",
    "class TransformerEncoder(nn.Module):\n",
    "    r\"\"\"One TransformerEncoder consists of several TransformerLayers.\n",
    "    Args:\n",
    "        n_layers(num): num of transformer layers in transformer encoder. Default: 2\n",
    "        n_heads(num): num of attention heads for multi-head attention layer. Default: 2\n",
    "        hidden_size(num): the input and output hidden size. Default: 64\n",
    "        inner_size(num): the dimensionality in feed-forward layer. Default: 256\n",
    "        hidden_dropout_prob(float): probability of an element to be zeroed. Default: 0.5\n",
    "        attn_dropout_prob(float): probability of an attention score to be zeroed. Default: 0.5\n",
    "        hidden_act(str): activation function in feed-forward layer. Default: 'gelu'\n",
    "                      candidates: 'gelu', 'relu', 'swish', 'tanh', 'sigmoid'\n",
    "        layer_norm_eps(float): a value added to the denominator for numerical stability. Default: 1e-12\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_layers=2,\n",
    "        n_heads=2,\n",
    "        hidden_size=64,\n",
    "        inner_size=256,\n",
    "        hidden_dropout_prob=0.5,\n",
    "        attn_dropout_prob=0.5,\n",
    "        hidden_act=\"gelu\",\n",
    "        layer_norm_eps=1e-12,\n",
    "    ):\n",
    "\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        layer = TransformerLayer(\n",
    "            n_heads,\n",
    "            hidden_size,\n",
    "            inner_size,\n",
    "            hidden_dropout_prob,\n",
    "            attn_dropout_prob,\n",
    "            hidden_act,\n",
    "            layer_norm_eps,\n",
    "        )\n",
    "        self.layer = nn.ModuleList([copy.deepcopy(layer) for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask, output_all_encoded_layers=True,\n",
    "                return_explanations=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states (torch.Tensor): the input of the TransformerEncoder\n",
    "            attention_mask (torch.Tensor): the attention mask for the input hidden_states\n",
    "            output_all_encoded_layers (Bool): whether output all transformer layers' output\n",
    "        Returns:\n",
    "            all_encoder_layers (list): if output_all_encoded_layers is True, return a list consists of all transformer\n",
    "            layers' output, otherwise return a list only consists of the output of last transformer layer.\n",
    "        \"\"\"\n",
    "        all_encoder_layers = []\n",
    "        for idx, layer_module in enumerate(self.layer):\n",
    "            \n",
    "            if return_explanations:\n",
    "                hidden_states, expl = layer_module(hidden_states, attention_mask, \n",
    "                                         return_explanations=return_explanations)\n",
    "            else:            \n",
    "                hidden_states = layer_module(hidden_states, attention_mask, \n",
    "                                             return_explanations=return_explanations)\n",
    "            if output_all_encoded_layers:\n",
    "                all_encoder_layers.append(hidden_states)\n",
    "        if not output_all_encoded_layers:\n",
    "            all_encoder_layers.append(hidden_states)\n",
    "            \n",
    "        if return_explanations:\n",
    "            return all_encoder_layers, expl\n",
    "        else:\n",
    "            return all_encoder_layers\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "class BERT4Rec(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_items, hidden_size, mask_ratio):\n",
    "        super(BERT4Rec, self).__init__()\n",
    "\n",
    "        self.n_layers = 2\n",
    "        self.n_heads = 2\n",
    "        self.hidden_size = hidden_size  \n",
    "        self.inner_size = 128 \n",
    "        self.hidden_dropout_prob = 0.2\n",
    "        self.attn_dropout_prob = 0.2\n",
    "        self.hidden_act = 'sigmoid'\n",
    "        self.layer_norm_eps = 1e-5\n",
    "        self.ITEM_SEQ = 'seq_i'\n",
    "        self.ITEM_SEQ_LEN = 'seq_len'\n",
    "        self.max_seq_length = 200\n",
    "        \n",
    "\n",
    "        self.mask_ratio = mask_ratio\n",
    "\n",
    "        self.loss_type =  'CE'\n",
    "        self.initializer_range = 1e-2\n",
    "\n",
    "        self.n_items = n_items\n",
    "        self.mask_token = self.n_items \n",
    "        self.mask_item_length = int(self.mask_ratio * self.max_seq_length)\n",
    "\n",
    "        self.item_embedding = nn.Embedding(self.n_items + 1, self.hidden_size, padding_idx=0) \n",
    "        self.position_embedding = nn.Embedding(self.max_seq_length + 1, self.hidden_size)  \n",
    "        self.trm_encoder = TransformerEncoder(\n",
    "            n_layers=self.n_layers,\n",
    "            n_heads=self.n_heads,\n",
    "            hidden_size=self.hidden_size,\n",
    "            inner_size=self.inner_size,\n",
    "            hidden_dropout_prob=self.hidden_dropout_prob,\n",
    "            attn_dropout_prob=self.attn_dropout_prob,\n",
    "            hidden_act=self.hidden_act,\n",
    "            layer_norm_eps=self.layer_norm_eps\n",
    "        )\n",
    "\n",
    "        self.LayerNorm = nn.LayerNorm(self.hidden_size, eps=self.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(self.hidden_dropout_prob)\n",
    "\n",
    "        try:\n",
    "            assert self.loss_type in ['BPR', 'CE']\n",
    "        except AssertionError:\n",
    "            raise AssertionError(\"Make sure 'loss_type' in ['BPR', 'CE']!\")\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def gather_indexes(self, output, gather_index):\n",
    "        \"\"\"Gathers the vectors at the specific positions over a minibatch\"\"\"\n",
    "        gather_index = gather_index.view(-1, 1, 1).expand(-1, -1, output.shape[-1])\n",
    "        output_tensor = output.gather(dim=1, index=gather_index)\n",
    "        return output_tensor.squeeze(1)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\" Initialize the weights \"\"\"\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.initializer_range)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "            module.bias.data.zero_()\n",
    "\n",
    "    def get_attention_mask(self, item_seq):\n",
    "        \"\"\"Generate bidirectional attention mask for multi-head attention.\"\"\"\n",
    "        attention_mask = (item_seq > 0).long()\n",
    "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)  \n",
    "        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype) \n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "        return extended_attention_mask\n",
    "\n",
    "    def _neg_sample(self, item_set):\n",
    "        item = random.randint(1, self.n_items - 1)\n",
    "        while item in item_set:\n",
    "            item = random.randint(1, self.n_items - 1)\n",
    "        return item\n",
    "\n",
    "    def _padding_sequence(self, sequence, max_length):\n",
    "        pad_len = max_length - len(sequence)\n",
    "        sequence = [0] * pad_len + sequence\n",
    "        sequence = sequence[-max_length:]  \n",
    "        return sequence\n",
    "\n",
    "    def reconstruct_train_data(self, item_seq):\n",
    "        \"\"\"\n",
    "        Mask item sequence for training.\n",
    "        \"\"\"\n",
    "        device = item_seq.device\n",
    "        batch_size = item_seq.size(0)\n",
    "\n",
    "        sequence_instances = item_seq.cpu().numpy().tolist()\n",
    "\n",
    "        masked_item_sequence = []\n",
    "        pos_items = []\n",
    "        neg_items = []\n",
    "        masked_index = []\n",
    "        for instance in sequence_instances:\n",
    "            masked_sequence = instance.copy()\n",
    "            pos_item = []\n",
    "            neg_item = []\n",
    "            index_ids = []\n",
    "            for index_id, item in enumerate(instance):\n",
    "                if item == 0:\n",
    "                    break\n",
    "                prob = random.random()\n",
    "                if prob < self.mask_ratio:\n",
    "                    pos_item.append(item)\n",
    "                    neg_item.append(self._neg_sample(instance))\n",
    "                    masked_sequence[index_id] = self.mask_token\n",
    "                    index_ids.append(index_id)\n",
    "\n",
    "            masked_item_sequence.append(masked_sequence)\n",
    "            pos_items.append(self._padding_sequence(pos_item, self.mask_item_length))\n",
    "            neg_items.append(self._padding_sequence(neg_item, self.mask_item_length))\n",
    "            masked_index.append(self._padding_sequence(index_ids, self.mask_item_length))\n",
    "\n",
    "        masked_item_sequence = torch.tensor(masked_item_sequence, dtype=torch.long, device=device).view(batch_size, -1)\n",
    "        pos_items = torch.tensor(pos_items, dtype=torch.long, device=device).view(batch_size, -1)\n",
    "        neg_items = torch.tensor(neg_items, dtype=torch.long, device=device).view(batch_size, -1)\n",
    "        masked_index = torch.tensor(masked_index, dtype=torch.long, device=device).view(batch_size, -1)\n",
    "        return masked_item_sequence, pos_items, neg_items, masked_index\n",
    "\n",
    "    def reconstruct_test_data(self, item_seq, item_seq_len):\n",
    "        \"\"\"\n",
    "        Add mask token at the last position according to the lengths of item_seq\n",
    "        \"\"\"\n",
    "        padding = torch.zeros(item_seq.size(0), dtype=torch.long, device=item_seq.device)  \n",
    "        item_seq = torch.cat((item_seq, padding.unsqueeze(-1)), dim=-1)\n",
    "        for batch_id, last_position in enumerate(item_seq_len):\n",
    "            item_seq[batch_id][last_position] = self.mask_token\n",
    "        return item_seq\n",
    "\n",
    "    def forward(self, item_seq, return_explanations=False):\n",
    "        \n",
    "        position_ids = torch.arange(item_seq.size(1), dtype=torch.long, device=item_seq.device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand_as(item_seq)\n",
    "        position_embedding = self.position_embedding(position_ids)\n",
    "        \n",
    "        item_emb = self.item_embedding(item_seq)\n",
    "        input_emb = item_emb + position_embedding\n",
    "        input_emb = self.LayerNorm(input_emb)\n",
    "        input_emb = self.dropout(input_emb)\n",
    "        extended_attention_mask = self.get_attention_mask(item_seq)\n",
    "        if return_explanations:\n",
    "            trm_output, explanations = self.trm_encoder(input_emb, extended_attention_mask, output_all_encoded_layers=True,\n",
    "                                         return_explanations=return_explanations)\n",
    "        else:\n",
    "            trm_output = self.trm_encoder(input_emb, extended_attention_mask, output_all_encoded_layers=True,\n",
    "                                         return_explanations=return_explanations)\n",
    "            \n",
    "        output = trm_output[-1]\n",
    "        \n",
    "        if return_explanations:\n",
    "            return output, explanations\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def multi_hot_embed(self, masked_index, max_length):\n",
    "        \"\"\"\n",
    "        For memory, we only need calculate loss for masked position.\n",
    "        Generate a multi-hot vector to indicate the masked position for masked sequence, and then is used for\n",
    "        gathering the masked position hidden representation.\n",
    "        Examples:\n",
    "            sequence: [1 2 3 4 5]\n",
    "            masked_sequence: [1 mask 3 mask 5]\n",
    "            masked_index: [1, 3]\n",
    "            max_length: 5\n",
    "            multi_hot_embed: [[0 1 0 0 0], [0 0 0 1 0]]\n",
    "        \"\"\"\n",
    "        masked_index = masked_index.view(-1)\n",
    "        multi_hot = torch.zeros(masked_index.size(0), max_length, device=masked_index.device)\n",
    "        multi_hot[torch.arange(masked_index.size(0)), masked_index] = 1\n",
    "        return multi_hot\n",
    "\n",
    "    def calculate_loss(self, interaction):\n",
    "        item_seq = interaction[self.ITEM_SEQ].long()\n",
    "        masked_item_seq, pos_items, neg_items, masked_index = self.reconstruct_train_data(item_seq)\n",
    "\n",
    "        seq_output = self.forward(masked_item_seq)\n",
    "        pred_index_map = self.multi_hot_embed(masked_index, masked_item_seq.size(-1))\n",
    "        pred_index_map = pred_index_map.view(masked_index.size(0), masked_index.size(1), -1)  \n",
    "        seq_output = torch.bmm(pred_index_map, seq_output) \n",
    "\n",
    "        if self.loss_type == 'BPR':\n",
    "            pos_items_emb = self.item_embedding(pos_items)  \n",
    "            neg_items_emb = self.item_embedding(neg_items)  \n",
    "            pos_score = torch.sum(seq_output * pos_items_emb, dim=-1)  \n",
    "            neg_score = torch.sum(seq_output * neg_items_emb, dim=-1)  \n",
    "            targets = (masked_index > 0).float()\n",
    "            loss = - torch.sum(torch.log(1e-14 + torch.sigmoid(pos_score - neg_score)) * targets) \\\n",
    "                   / torch.sum(targets)\n",
    "            return loss\n",
    "\n",
    "        elif self.loss_type == 'CE':\n",
    "            loss_fct = nn.CrossEntropyLoss(reduction='none')\n",
    "            test_item_emb = self.item_embedding.weight[:self.n_items] \n",
    "            logits = torch.matmul(seq_output, test_item_emb.transpose(0, 1)) \n",
    "            targets = (masked_index > 0).float().view(-1) \n",
    "\n",
    "            loss = torch.sum(loss_fct(logits.view(-1, test_item_emb.size(0)), pos_items.view(-1)) * targets) \\\n",
    "                   / torch.sum(targets)\n",
    "            return loss\n",
    "        else:\n",
    "            raise NotImplementedError(\"Make sure 'loss_type' in ['BPR', 'CE']!\")\n",
    "\n",
    "\n",
    "    def full_sort_predict(self, interaction, return_explanations=False):\n",
    "        item_seq = interaction[self.ITEM_SEQ].long()\n",
    "        item_seq_len = interaction[self.ITEM_SEQ_LEN].long()\n",
    "        item_seq = self.reconstruct_test_data(item_seq, item_seq_len)\n",
    "        \n",
    "        if return_explanations:\n",
    "            seq_output, expl = self.forward(item_seq, return_explanations=return_explanations)\n",
    "        else:\n",
    "            seq_output = self.forward(item_seq, return_explanations=return_explanations)\n",
    "            \n",
    "        \n",
    "        seq_output = self.gather_indexes(seq_output, item_seq_len - 1)  \n",
    "        test_items_emb = self.item_embedding.weight[:self.n_items]  \n",
    "        scores = torch.matmul(seq_output, test_items_emb.transpose(0, 1))  \n",
    "                \n",
    "        idxs = item_seq.nonzero()\n",
    "        item_seq[item_seq==self.n_items] = 0\n",
    "        scores[idxs[:,0], item_seq[idxs[:,0],idxs[:,1]].long()] = -1000\n",
    "\n",
    "        if return_explanations:\n",
    "            return scores, expl\n",
    "        else:\n",
    "            return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7572cb9-71db-40e3-8a75-edc88f89c4a4",
   "metadata": {},
   "source": [
    "### Hours only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f85b13f4-8657-4ded-a877-b7be251bc6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextBERT4Rec(BERT4Rec):\n",
    "\n",
    "    def __init__(self, n_items, hidden_size, mask_ratio):\n",
    "        super(BERT4Rec, self).__init__()\n",
    "        \n",
    "        self.n_layers = 2\n",
    "        self.n_heads = 2\n",
    "        self.hidden_size = hidden_size  \n",
    "        self.inner_size = 128 \n",
    "        self.hidden_dropout_prob = 0.2\n",
    "        self.attn_dropout_prob = 0.2\n",
    "        self.hidden_act = 'sigmoid'\n",
    "        self.layer_norm_eps = 1e-5\n",
    "        self.ITEM_SEQ = 'seq_i'\n",
    "        self.ITEM_SEQ_LEN = 'seq_len'\n",
    "        self.max_seq_length = 200\n",
    "        \n",
    "\n",
    "        self.mask_ratio = mask_ratio\n",
    "\n",
    "        self.loss_type =  'CE'\n",
    "        self.initializer_range = 1e-2\n",
    "\n",
    "        # load dataset info\n",
    "        self.n_items = n_items\n",
    "        self.mask_token = self.n_items\n",
    "        self.mask_item_length = int(self.mask_ratio * self.max_seq_length)\n",
    "\n",
    "        # define layers and loss\n",
    "        self.hours_embedding = nn.Embedding(24, self.hidden_size)\n",
    "        self.item_embedding = nn.Embedding(self.n_items + 1, self.hidden_size, padding_idx=0)  # mask token add 1\n",
    "        self.position_embedding = nn.Embedding(self.max_seq_length + 1, self.hidden_size)  # add mask_token at the last\n",
    "        self.trm_encoder = TransformerEncoder(\n",
    "            n_layers=self.n_layers,\n",
    "            n_heads=self.n_heads,\n",
    "            hidden_size=self.hidden_size,\n",
    "            inner_size=self.inner_size,\n",
    "            hidden_dropout_prob=self.hidden_dropout_prob,\n",
    "            attn_dropout_prob=self.attn_dropout_prob,\n",
    "            hidden_act=self.hidden_act,\n",
    "            layer_norm_eps=self.layer_norm_eps\n",
    "        )\n",
    "\n",
    "        self.LayerNorm = nn.LayerNorm(self.hidden_size, eps=self.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(self.hidden_dropout_prob)\n",
    "\n",
    "        try:\n",
    "            assert self.loss_type in ['BPR', 'CE']\n",
    "        except AssertionError:\n",
    "            raise AssertionError(\"Make sure 'loss_type' in ['BPR', 'CE']!\")\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "\n",
    "\n",
    "    def reconstruct_test_data(self,\n",
    "                              item_seq,\n",
    "                              item_seq_len,\n",
    "                              hours,\n",
    "                              hours_valid,\n",
    "                              particular_day=-1,\n",
    "                              ):\n",
    "        \"\"\"\n",
    "        Add mask token at the last position according to the lengths of item_seq\n",
    "        \"\"\"\n",
    "        padding = torch.zeros(item_seq.size(0), dtype=torch.long, device=item_seq.device)  # [B]\n",
    "        item_seq = torch.cat((item_seq, padding.unsqueeze(-1)), dim=-1)  # [B max_len+1]\n",
    "        hours = torch.cat((hours, padding.unsqueeze(-1)), dim=-1)\n",
    "        for batch_id, last_position in enumerate(item_seq_len):\n",
    "            item_seq[batch_id][last_position] = self.mask_token\n",
    "            if particular_day == -1:\n",
    "                hours[batch_id][last_position] = hours_valid[batch_id]\n",
    "            else:\n",
    "                hours[batch_id][last_position] = particular_day\n",
    "        return item_seq, hours\n",
    "\n",
    "    def forward(self, item_seq, hours, return_explanations=False):\n",
    "        \n",
    "        \n",
    "        hours_embeddings = self.hours_embedding(hours.long())\n",
    "        \n",
    "        position_ids = torch.arange(item_seq.size(1), dtype=torch.long, device=item_seq.device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand_as(item_seq)\n",
    "        position_embedding = self.position_embedding(position_ids)\n",
    "        item_emb = self.item_embedding(item_seq)\n",
    "        input_emb = item_emb + position_embedding + hours_embeddings\n",
    "        input_emb = self.LayerNorm(input_emb)\n",
    "        input_emb = self.dropout(input_emb)\n",
    "        extended_attention_mask = self.get_attention_mask(item_seq)\n",
    "        if return_explanations:\n",
    "            trm_output, explanations = self.trm_encoder(input_emb, extended_attention_mask, output_all_encoded_layers=True,\n",
    "                                         return_explanations=return_explanations)\n",
    "        else:\n",
    "            trm_output = self.trm_encoder(input_emb, extended_attention_mask, output_all_encoded_layers=True,\n",
    "                                         return_explanations=return_explanations)\n",
    "            \n",
    "        output = trm_output[-1]\n",
    "        \n",
    "        if return_explanations:\n",
    "            return output, explanations\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "\n",
    "    def calculate_loss(self, interaction):\n",
    "        item_seq = interaction[self.ITEM_SEQ].long()\n",
    "        masked_item_seq, pos_items, neg_items, masked_index = self.reconstruct_train_data(item_seq)\n",
    "\n",
    "        seq_output = self.forward(masked_item_seq, hours=interaction['hours'])\n",
    "        pred_index_map = self.multi_hot_embed(masked_index, masked_item_seq.size(-1))  \n",
    "        pred_index_map = pred_index_map.view(masked_index.size(0), masked_index.size(1), -1) \n",
    "        seq_output = torch.bmm(pred_index_map, seq_output) \n",
    "\n",
    "        if self.loss_type == 'BPR':\n",
    "            pos_items_emb = self.item_embedding(pos_items)  \n",
    "            neg_items_emb = self.item_embedding(neg_items)  \n",
    "            pos_score = torch.sum(seq_output * pos_items_emb, dim=-1)  \n",
    "            neg_score = torch.sum(seq_output * neg_items_emb, dim=-1)  \n",
    "            targets = (masked_index > 0).float()\n",
    "            loss = - torch.sum(torch.log(1e-14 + torch.sigmoid(pos_score - neg_score)) * targets) \\\n",
    "                   / torch.sum(targets)\n",
    "            return loss\n",
    "\n",
    "        elif self.loss_type == 'CE':\n",
    "            loss_fct = nn.CrossEntropyLoss(reduction='none')\n",
    "            test_item_emb = self.item_embedding.weight[:self.n_items]  \n",
    "            logits = torch.matmul(seq_output, test_item_emb.transpose(0, 1))  \n",
    "            targets = (masked_index > 0).float().view(-1)  \n",
    "\n",
    "            loss = torch.sum(loss_fct(logits.view(-1, test_item_emb.size(0)), pos_items.view(-1)) * targets) \\\n",
    "                   / torch.sum(targets)\n",
    "            return loss\n",
    "        else:\n",
    "            raise NotImplementedError(\"Make sure 'loss_type' in ['BPR', 'CE']!\")\n",
    "\n",
    "\n",
    "    def full_sort_predict(self, \n",
    "                          interaction,\n",
    "                          return_explanations=False,\n",
    "                          particular_day=-1):\n",
    "        \n",
    "        item_seq = interaction[self.ITEM_SEQ].long()\n",
    "        item_seq_len = interaction[self.ITEM_SEQ_LEN].long()\n",
    "        item_seq, hours = self.reconstruct_test_data(item_seq,\n",
    "                                              item_seq_len,\n",
    "                                              hours=interaction['hours'],\n",
    "                                              hours_valid=interaction['hours_valid'].long(),\n",
    "                                              particular_day=particular_day)\n",
    "        \n",
    "        \n",
    "        if return_explanations:\n",
    "            seq_output, expl = self.forward(item_seq,\n",
    "                                            hours=hours,\n",
    "                                            return_explanations=return_explanations)\n",
    "        else:\n",
    "            seq_output = self.forward(item_seq,\n",
    "                                      hours=hours,\n",
    "                                      return_explanations=return_explanations)\n",
    "            \n",
    "        \n",
    "        seq_output = self.gather_indexes(seq_output, item_seq_len - 1)  \n",
    "        test_items_emb = self.item_embedding.weight[:self.n_items]  \n",
    "        scores = torch.matmul(seq_output, test_items_emb.transpose(0, 1)) \n",
    "                \n",
    "        idxs = item_seq.nonzero()\n",
    "        item_seq[item_seq==self.n_items] = 0\n",
    "        scores[idxs[:,0], item_seq[idxs[:,0],idxs[:,1]].long()] = -1000\n",
    "\n",
    "        if return_explanations:\n",
    "            return scores, expl\n",
    "        else:\n",
    "            return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d9829b27",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/100 * Epoch (train): 100%|| 24/24 [00:28<00:00,  1.20s/it, _timer/_fps=186.389, _timer/batch_time=0.815, _timer/data_time=0.453, _timer/model_time=0.363, loss=7.394, lr=0.010, map10=0.000e+00, momentum=0.900, ndcg20=0.000e+00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (1/100) loss: 7.5486999454877255 | lr: 0.01 | map10: 0.0 | map10/std: 0.0 | momentum: 0.9 | ndcg20: 0.0 | ndcg20/std: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/100 * Epoch (valid): 100%|| 24/24 [00:13<00:00,  1.72it/s, _timer/_fps=429.411, _timer/batch_time=0.354, _timer/data_time=0.020, _timer/model_time=0.334, loss=7.326, lr=0.010, map10=0.103, momentum=0.900, ndcg20=0.060]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid (1/100) loss: 7.4432442595627135 | lr: 0.01 | map10: 0.12408531623960332 | map10/std: 0.016438783373056175 | momentum: 0.9 | ndcg20: 0.06770167051265572 | ndcg20/std: 0.006908426776674754\n",
      "* Epoch (1/100) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2/100 * Epoch (train): 100%|| 24/24 [00:28<00:00,  1.17s/it, _timer/_fps=178.674, _timer/batch_time=0.851, _timer/data_time=0.492, _timer/model_time=0.359, loss=7.353, lr=0.010, map10=0.000e+00, momentum=0.900, ndcg20=0.000e+00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (2/100) loss: 7.453495850468314 | lr: 0.01 | map10: 0.0 | map10/std: 0.0 | momentum: 0.9 | ndcg20: 0.0 | ndcg20/std: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2/100 * Epoch (valid): 100%|| 24/24 [00:13<00:00,  1.84it/s, _timer/_fps=444.299, _timer/batch_time=0.342, _timer/data_time=0.020, _timer/model_time=0.322, loss=7.320, lr=0.010, map10=0.109, momentum=0.900, ndcg20=0.067]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid (2/100) loss: 7.41002306717121 | lr: 0.01 | map10: 0.13553662106690814 | map10/std: 0.016741590569495545 | momentum: 0.9 | ndcg20: 0.0768625318609326 | ndcg20/std: 0.007651711078039143\n",
      "* Epoch (2/100) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3/100 * Epoch (train): 100%|| 24/24 [00:28<00:00,  1.17s/it, _timer/_fps=167.814, _timer/batch_time=0.906, _timer/data_time=0.499, _timer/model_time=0.406, loss=7.354, lr=0.010, map10=0.000e+00, momentum=0.900, ndcg20=0.000e+00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (3/100) loss: 7.42241380514688 | lr: 0.01 | map10: 0.0 | map10/std: 0.0 | momentum: 0.9 | ndcg20: 0.0 | ndcg20/std: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3/100 * Epoch (valid): 100%|| 24/24 [00:13<00:00,  1.79it/s, _timer/_fps=424.846, _timer/batch_time=0.358, _timer/data_time=0.019, _timer/model_time=0.338, loss=7.316, lr=0.010, map10=0.108, momentum=0.900, ndcg20=0.067]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid (3/100) loss: 7.383084136760787 | lr: 0.01 | map10: 0.1363276656100292 | map10/std: 0.017371114096854018 | momentum: 0.9 | ndcg20: 0.07431822904687843 | ndcg20/std: 0.007529737098835713\n",
      "* Epoch (3/100) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4/100 * Epoch (train): 100%|| 24/24 [00:30<00:00,  1.28s/it, _timer/_fps=171.024, _timer/batch_time=0.889, _timer/data_time=0.506, _timer/model_time=0.382, loss=7.348, lr=0.010, map10=0.000e+00, momentum=0.900, ndcg20=0.000e+00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (4/100) loss: 7.417258884101513 | lr: 0.01 | map10: 0.0 | map10/std: 0.0 | momentum: 0.9 | ndcg20: 0.0 | ndcg20/std: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4/100 * Epoch (valid): 100%|| 24/24 [00:15<00:00,  1.59it/s, _timer/_fps=453.082, _timer/batch_time=0.335, _timer/data_time=0.021, _timer/model_time=0.315, loss=7.317, lr=0.010, map10=0.117, momentum=0.900, ndcg20=0.066]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid (4/100) loss: 7.369235244018354 | lr: 0.01 | map10: 0.13686595531488888 | map10/std: 0.017472520268054098 | momentum: 0.9 | ndcg20: 0.07650183581358551 | ndcg20/std: 0.00782102264951083\n",
      "* Epoch (4/100) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5/100 * Epoch (train): 100%|| 24/24 [00:27<00:00,  1.16s/it, _timer/_fps=174.569, _timer/batch_time=0.871, _timer/data_time=0.513, _timer/model_time=0.358, loss=7.367, lr=0.010, map10=0.000e+00, momentum=0.900, ndcg20=0.000e+00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (5/100) loss: 7.3922384843131566 | lr: 0.01 | map10: 0.0 | map10/std: 0.0 | momentum: 0.9 | ndcg20: 0.0 | ndcg20/std: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5/100 * Epoch (valid): 100%|| 24/24 [00:13<00:00,  1.76it/s, _timer/_fps=349.308, _timer/batch_time=0.435, _timer/data_time=0.023, _timer/model_time=0.412, loss=7.341, lr=0.010, map10=0.125, momentum=0.900, ndcg20=0.066]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid (5/100) loss: 7.354217925292768 | lr: 0.01 | map10: 0.14151358155422655 | map10/std: 0.016127745767677365 | momentum: 0.9 | ndcg20: 0.07895260493684289 | ndcg20/std: 0.007867150151322277\n",
      "* Epoch (5/100) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6/100 * Epoch (train): 100%|| 24/24 [00:29<00:00,  1.23s/it, _timer/_fps=183.078, _timer/batch_time=0.830, _timer/data_time=0.471, _timer/model_time=0.359, loss=7.357, lr=0.010, map10=0.000e+00, momentum=0.900, ndcg20=0.000e+00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (6/100) loss: 7.395082389124181 | lr: 0.01 | map10: 0.0 | map10/std: 0.0 | momentum: 0.9 | ndcg20: 0.0 | ndcg20/std: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6/100 * Epoch (valid): 100%|| 24/24 [00:12<00:00,  1.85it/s, _timer/_fps=460.874, _timer/batch_time=0.330, _timer/data_time=0.020, _timer/model_time=0.310, loss=7.333, lr=0.010, map10=0.114, momentum=0.900, ndcg20=0.067]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid (6/100) loss: 7.359322107706638 | lr: 0.01 | map10: 0.1329105152298283 | map10/std: 0.016020637968934704 | momentum: 0.9 | ndcg20: 0.07776578152613925 | ndcg20/std: 0.007403739742993672\n",
      "* Epoch (6/100) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7/100 * Epoch (train): 100%|| 24/24 [00:28<00:00,  1.19s/it, _timer/_fps=181.845, _timer/batch_time=0.836, _timer/data_time=0.475, _timer/model_time=0.361, loss=7.381, lr=0.010, map10=0.000e+00, momentum=0.900, ndcg20=0.000e+00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (7/100) loss: 7.399077822830503 | lr: 0.01 | map10: 0.0 | map10/std: 0.0 | momentum: 0.9 | ndcg20: 0.0 | ndcg20/std: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7/100 * Epoch (valid): 100%|| 24/24 [00:12<00:00,  1.86it/s, _timer/_fps=453.320, _timer/batch_time=0.335, _timer/data_time=0.019, _timer/model_time=0.316, loss=7.359, lr=0.010, map10=0.116, momentum=0.900, ndcg20=0.070]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid (7/100) loss: 7.361966505113816 | lr: 0.01 | map10: 0.1315164936101989 | map10/std: 0.01879966374652992 | momentum: 0.9 | ndcg20: 0.07702410432281874 | ndcg20/std: 0.008275508973209202\n",
      "* Epoch (7/100) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8/100 * Epoch (train): 100%|| 24/24 [00:26<00:00,  1.12s/it, _timer/_fps=183.215, _timer/batch_time=0.830, _timer/data_time=0.467, _timer/model_time=0.363, loss=7.363, lr=0.010, map10=0.000e+00, momentum=0.900, ndcg20=0.000e+00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (8/100) loss: 7.3877011090714415 | lr: 0.01 | map10: 0.0 | map10/std: 0.0 | momentum: 0.9 | ndcg20: 0.0 | ndcg20/std: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8/100 * Epoch (valid): 100%|| 24/24 [00:12<00:00,  1.88it/s, _timer/_fps=395.976, _timer/batch_time=0.384, _timer/data_time=0.021, _timer/model_time=0.363, loss=7.343, lr=0.010, map10=0.111, momentum=0.900, ndcg20=0.070]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid (8/100) loss: 7.354840564727785 | lr: 0.01 | map10: 0.138475516339801 | map10/std: 0.0185681068924728 | momentum: 0.9 | ndcg20: 0.07955095825211102 | ndcg20/std: 0.008095980176571925\n",
      "* Epoch (8/100) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9/100 * Epoch (train): 100%|| 24/24 [00:28<00:00,  1.18s/it, _timer/_fps=164.803, _timer/batch_time=0.922, _timer/data_time=0.557, _timer/model_time=0.365, loss=7.344, lr=0.010, map10=0.000e+00, momentum=0.900, ndcg20=0.000e+00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (9/100) loss: 7.392812946774312 | lr: 0.01 | map10: 0.0 | map10/std: 0.0 | momentum: 0.9 | ndcg20: 0.0 | ndcg20/std: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9/100 * Epoch (valid): 100%|| 24/24 [00:12<00:00,  1.89it/s, _timer/_fps=468.594, _timer/batch_time=0.324, _timer/data_time=0.019, _timer/model_time=0.305, loss=7.325, lr=0.010, map10=0.116, momentum=0.900, ndcg20=0.066]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid (9/100) loss: 7.359860982326483 | lr: 0.01 | map10: 0.13974313579055647 | map10/std: 0.01689711623110146 | momentum: 0.9 | ndcg20: 0.08036795160825679 | ndcg20/std: 0.008032918983786844\n",
      "* Epoch (9/100) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/100 * Epoch (train): 100%|| 24/24 [00:26<00:00,  1.12s/it, _timer/_fps=166.698, _timer/batch_time=0.912, _timer/data_time=0.546, _timer/model_time=0.366, loss=7.337, lr=0.010, map10=0.000e+00, momentum=0.900, ndcg20=0.000e+00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (10/100) loss: 7.389346378528519 | lr: 0.01 | map10: 0.0 | map10/std: 0.0 | momentum: 0.9 | ndcg20: 0.0 | ndcg20/std: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/100 * Epoch (valid): 100%|| 24/24 [00:12<00:00,  1.85it/s, _timer/_fps=454.407, _timer/batch_time=0.335, _timer/data_time=0.020, _timer/model_time=0.314, loss=7.313, lr=0.010, map10=0.124, momentum=0.900, ndcg20=0.071]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid (10/100) loss: 7.355596533516385 | lr: 0.01 | map10: 0.1414352862250726 | map10/std: 0.017744710421119393 | momentum: 0.9 | ndcg20: 0.07977344540768111 | ndcg20/std: 0.007730465737714401\n",
      "* Epoch (10/100) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = ContextBERT4Rec(n_items=len(item2idx)+1, mask_ratio=0.2, hidden_size=128)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "lr_scheduler = StepLR(optimizer, step_size=20, gamma=0.1)\n",
    "engine = dl.DeviceEngine('cpu')\n",
    "hparams = {\n",
    "    \"anneal_cap\": 0.2,\n",
    "    \"total_anneal_steps\": 6000,\n",
    "}\n",
    "\n",
    "\n",
    "callbacks = [\n",
    "    dl.NDCGCallback(\"logits\", \"targets\", [20]),\n",
    "    dl.MAPCallback(\"logits\", \"targets\", [10]),\n",
    "    dl.OptimizerCallback(\"loss\", accumulation_steps=1),\n",
    "    dl.EarlyStoppingCallback(\n",
    "        patience=5, loader_key=\"valid\", metric_key=\"map10\", minimize=False\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "runner = RecSysRunner()\n",
    "runner.train(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    engine=engine,\n",
    "    hparams=hparams,\n",
    "    scheduler=lr_scheduler,\n",
    "    loaders=loaders,\n",
    "    num_epochs=100,\n",
    "    verbose=True,\n",
    "    timeit=True,\n",
    "    callbacks=callbacks,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1db4b99e-9aac-4917-bc5c-117c96560b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [00:06, 15.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6040\n"
     ]
    }
   ],
   "source": [
    "test_dataset = MyDataset(ds=joined, num_items=n_items, phase='test',item2idx=item2idx)\n",
    "\n",
    "\n",
    "inference_loader = DataLoader(test_dataset, \n",
    "                              batch_size=joined.shape[0]//100, \n",
    "                              collate_fn=collate_fn_train,)\n",
    "\n",
    "preds = []\n",
    "\n",
    "for prediction in tqdm(runner.predict_loader(loader=inference_loader)):\n",
    "    preds.extend(prediction.detach().cpu().numpy().tolist())\n",
    "    \n",
    "print(len(preds))\n",
    "assert len(preds) == joined.shape[0]\n",
    "\n",
    "joined['preds_contextbert4rec'] = preds\n",
    "joined['recs_contextbert4rec_10'] = joined['preds_contextbert4rec'].apply(lambda x: np.argsort(-np.array(x))[:10])\n",
    "joined['recs_contextbert4rec_10'] = joined['recs_contextbert4rec_10'].apply(lambda x: [idx2item[t-1] for t in x])\n",
    "joined['recs_contextbert4rec_5'] = joined['preds_contextbert4rec'].apply(lambda x: np.argsort(-np.array(x))[:5])\n",
    "joined['recs_contextbert4rec_5'] = joined['recs_contextbert4rec_5'].apply(lambda x: [idx2item[t-1] for t in x])\n",
    "joined.drop(['preds_contextbert4rec'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "713d87bd-6029-4aea-b3d7-c2852c8750ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ndcg': 0.15241325770658687, 'recall': 0.03240332694747826}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_recommender(joined, model_preds='recs_contextbert4rec_10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db54fead-efb0-48ac-8159-0b2a6ceef72e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ndcg': 0.09340676542190061, 'recall': 0.01696575075374235}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_recommender(joined, model_preds='recs_contextbert4rec_5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853654ea-8f4e-4af4-b22d-efbcc874b46f",
   "metadata": {},
   "source": [
    "### Hours + day of the week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97a816d9-99b5-4ca3-ab20-1f599d7a31c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextBERT4Rec(BERT4Rec):\n",
    "\n",
    "    def __init__(self, n_items, hidden_size, mask_ratio):\n",
    "        super(BERT4Rec, self).__init__()\n",
    "        \n",
    "        self.n_layers = 2\n",
    "        self.n_heads = 2\n",
    "        self.hidden_size = hidden_size\n",
    "        self.inner_size = 128 \n",
    "        self.hidden_dropout_prob = 0.2\n",
    "        self.attn_dropout_prob = 0.2\n",
    "        self.hidden_act = 'sigmoid'\n",
    "        self.layer_norm_eps = 1e-5\n",
    "        self.ITEM_SEQ = 'seq_i'\n",
    "        self.ITEM_SEQ_LEN = 'seq_len'\n",
    "        self.max_seq_length = 200\n",
    "        \n",
    "\n",
    "        self.mask_ratio = mask_ratio\n",
    "\n",
    "        self.loss_type =  'CE'\n",
    "        self.initializer_range = 1e-2\n",
    "\n",
    "        # load dataset info\n",
    "        self.n_items = n_items\n",
    "        self.mask_token = self.n_items\n",
    "        self.mask_item_length = int(self.mask_ratio * self.max_seq_length)\n",
    "\n",
    "        # define layers and loss\n",
    "        self.weekday_embedding = nn.Embedding(7, self.hidden_size)\n",
    "        self.hours_embedding = nn.Embedding(24, self.hidden_size)\n",
    "        self.item_embedding = nn.Embedding(self.n_items + 1, self.hidden_size, padding_idx=0)  \n",
    "        self.position_embedding = nn.Embedding(self.max_seq_length + 1, self.hidden_size) \n",
    "        self.trm_encoder = TransformerEncoder(\n",
    "            n_layers=self.n_layers,\n",
    "            n_heads=self.n_heads,\n",
    "            hidden_size=self.hidden_size,\n",
    "            inner_size=self.inner_size,\n",
    "            hidden_dropout_prob=self.hidden_dropout_prob,\n",
    "            attn_dropout_prob=self.attn_dropout_prob,\n",
    "            hidden_act=self.hidden_act,\n",
    "            layer_norm_eps=self.layer_norm_eps\n",
    "        )\n",
    "\n",
    "        self.LayerNorm = nn.LayerNorm(self.hidden_size, eps=self.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(self.hidden_dropout_prob)\n",
    "\n",
    "        try:\n",
    "            assert self.loss_type in ['BPR', 'CE']\n",
    "        except AssertionError:\n",
    "            raise AssertionError(\"Make sure 'loss_type' in ['BPR', 'CE']!\")\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "\n",
    "\n",
    "    def reconstruct_test_data(self,\n",
    "                              item_seq,\n",
    "                              item_seq_len,\n",
    "                              dow,\n",
    "                              hours,\n",
    "                              dow_valid,\n",
    "                              hours_valid,\n",
    "                              particular_day=-1,\n",
    "                              ):\n",
    "        \"\"\"\n",
    "        Add mask token at the last position according to the lengths of item_seq\n",
    "        \"\"\"\n",
    "        padding = torch.zeros(item_seq.size(0), dtype=torch.long, device=item_seq.device)  \n",
    "        item_seq = torch.cat((item_seq, padding.unsqueeze(-1)), dim=-1)  \n",
    "        dow = torch.cat((dow, padding.unsqueeze(-1)), dim=-1)\n",
    "        hours = torch.cat((hours, padding.unsqueeze(-1)), dim=-1)\n",
    "        for batch_id, last_position in enumerate(item_seq_len):\n",
    "            item_seq[batch_id][last_position] = self.mask_token\n",
    "            if particular_day == -1:\n",
    "                dow[batch_id][last_position] = dow_valid[batch_id]\n",
    "                hours[batch_id][last_position] = hours_valid[batch_id]\n",
    "            else:\n",
    "                dow[batch_id][last_position] = particular_day\n",
    "                hours[batch_id][last_position] = particular_day\n",
    "        return item_seq, dow, hours\n",
    "\n",
    "    def forward(self, item_seq, dow, hours, return_explanations=False):\n",
    "        \n",
    "        \n",
    "        dow_embeddings = self.weekday_embedding(dow.long())\n",
    "        hours_embeddings = self.hours_embedding(hours.long())\n",
    "        \n",
    "        position_ids = torch.arange(item_seq.size(1), dtype=torch.long, device=item_seq.device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand_as(item_seq)\n",
    "        position_embedding = self.position_embedding(position_ids)\n",
    "        item_emb = self.item_embedding(item_seq)\n",
    "        input_emb = item_emb + position_embedding + dow_embeddings + hours_embeddings\n",
    "        input_emb = self.LayerNorm(input_emb)\n",
    "        input_emb = self.dropout(input_emb)\n",
    "        extended_attention_mask = self.get_attention_mask(item_seq)\n",
    "        if return_explanations:\n",
    "            trm_output, explanations = self.trm_encoder(input_emb, extended_attention_mask, output_all_encoded_layers=True,\n",
    "                                         return_explanations=return_explanations)\n",
    "        else:\n",
    "            trm_output = self.trm_encoder(input_emb, extended_attention_mask, output_all_encoded_layers=True,\n",
    "                                         return_explanations=return_explanations)\n",
    "            \n",
    "        output = trm_output[-1]\n",
    "        \n",
    "        if return_explanations:\n",
    "            return output, explanations\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "\n",
    "    def calculate_loss(self, interaction):\n",
    "        item_seq = interaction[self.ITEM_SEQ].long()\n",
    "        masked_item_seq, pos_items, neg_items, masked_index = self.reconstruct_train_data(item_seq)\n",
    "\n",
    "        seq_output = self.forward(masked_item_seq, dow=interaction['dow'], hours=interaction['hours'])\n",
    "        pred_index_map = self.multi_hot_embed(masked_index, masked_item_seq.size(-1)) \n",
    "        pred_index_map = pred_index_map.view(masked_index.size(0), masked_index.size(1), -1) \n",
    "        seq_output = torch.bmm(pred_index_map, seq_output)  \n",
    "\n",
    "        if self.loss_type == 'BPR':\n",
    "            pos_items_emb = self.item_embedding(pos_items) \n",
    "            neg_items_emb = self.item_embedding(neg_items)  \n",
    "            pos_score = torch.sum(seq_output * pos_items_emb, dim=-1)  \n",
    "            neg_score = torch.sum(seq_output * neg_items_emb, dim=-1) \n",
    "            targets = (masked_index > 0).float()\n",
    "            loss = - torch.sum(torch.log(1e-14 + torch.sigmoid(pos_score - neg_score)) * targets) \\\n",
    "                   / torch.sum(targets)\n",
    "            return loss\n",
    "\n",
    "        elif self.loss_type == 'CE':\n",
    "            loss_fct = nn.CrossEntropyLoss(reduction='none')\n",
    "            test_item_emb = self.item_embedding.weight[:self.n_items]  \n",
    "            logits = torch.matmul(seq_output, test_item_emb.transpose(0, 1))  \n",
    "            targets = (masked_index > 0).float().view(-1)  \n",
    "\n",
    "            loss = torch.sum(loss_fct(logits.view(-1, test_item_emb.size(0)), pos_items.view(-1)) * targets) \\\n",
    "                   / torch.sum(targets)\n",
    "            return loss\n",
    "        else:\n",
    "            raise NotImplementedError(\"Make sure 'loss_type' in ['BPR', 'CE']!\")\n",
    "\n",
    "\n",
    "    def full_sort_predict(self, \n",
    "                          interaction,\n",
    "                          return_explanations=False,\n",
    "                          particular_day=-1):\n",
    "        \n",
    "        item_seq = interaction[self.ITEM_SEQ].long()\n",
    "        item_seq_len = interaction[self.ITEM_SEQ_LEN].long()\n",
    "        item_seq, dow, hours = self.reconstruct_test_data(item_seq,\n",
    "                                              item_seq_len,\n",
    "                                              dow=interaction['dow'],\n",
    "                                              hours=interaction['hours'],\n",
    "                                              dow_valid=interaction['dow_valid'].long(),\n",
    "                                              hours_valid=interaction['hours_valid'].long(),\n",
    "                                              particular_day=particular_day)\n",
    "        \n",
    "        \n",
    "        if return_explanations:\n",
    "            seq_output, expl = self.forward(item_seq,\n",
    "                                            dow=dow,\n",
    "                                            hours=hours,\n",
    "                                            return_explanations=return_explanations)\n",
    "        else:\n",
    "            seq_output = self.forward(item_seq,\n",
    "                                      dow=dow,\n",
    "                                      hours=hours,\n",
    "                                      return_explanations=return_explanations)\n",
    "            \n",
    "        \n",
    "        seq_output = self.gather_indexes(seq_output, item_seq_len - 1)  \n",
    "        test_items_emb = self.item_embedding.weight[:self.n_items]  \n",
    "        scores = torch.matmul(seq_output, test_items_emb.transpose(0, 1))  \n",
    "                \n",
    "        idxs = item_seq.nonzero()\n",
    "        item_seq[item_seq==self.n_items] = 0\n",
    "        scores[idxs[:,0], item_seq[idxs[:,0],idxs[:,1]].long()] = -1000\n",
    "\n",
    "        if return_explanations:\n",
    "            return scores, expl\n",
    "        else:\n",
    "            return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b090836-06a0-4367-a0ef-bc5ea8176ee1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/100 * Epoch (train): 100%|| 24/24 [00:26<00:00,  1.10s/it, _timer/_fps=184.197, _timer/batch_time=0.825, _timer/data_time=0.465, _timer/model_time=0.360, loss=7.418, lr=0.010, map10=0.000e+00, momentum=0.900, ndcg20=0.000e+00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (1/100) loss: 7.555545104576263 | lr: 0.01 | map10: 0.0 | map10/std: 0.0 | momentum: 0.9 | ndcg20: 0.0 | ndcg20/std: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/100 * Epoch (valid): 100%|| 24/24 [00:12<00:00,  1.88it/s, _timer/_fps=462.453, _timer/batch_time=0.329, _timer/data_time=0.020, _timer/model_time=0.309, loss=7.341, lr=0.010, map10=0.111, momentum=0.900, ndcg20=0.064]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid (1/100) loss: 7.445513191602088 | lr: 0.01 | map10: 0.12790470888085714 | map10/std: 0.01621691434394232 | momentum: 0.9 | ndcg20: 0.06918315407850885 | ndcg20/std: 0.007077874368164376\n",
      "* Epoch (1/100) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2/100 * Epoch (train): 100%|| 24/24 [00:28<00:00,  1.21s/it, _timer/_fps=179.797, _timer/batch_time=0.845, _timer/data_time=0.455, _timer/model_time=0.390, loss=7.367, lr=0.010, map10=0.000e+00, momentum=0.900, ndcg20=0.000e+00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (2/100) loss: 7.44987665580598 | lr: 0.01 | map10: 0.0 | map10/std: 0.0 | momentum: 0.9 | ndcg20: 0.0 | ndcg20/std: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2/100 * Epoch (valid): 100%|| 24/24 [00:12<00:00,  1.89it/s, _timer/_fps=459.865, _timer/batch_time=0.331, _timer/data_time=0.020, _timer/model_time=0.311, loss=7.329, lr=0.010, map10=0.111, momentum=0.900, ndcg20=0.064]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid (2/100) loss: 7.406028172827715 | lr: 0.01 | map10: 0.13488951142852668 | map10/std: 0.01732436265849217 | momentum: 0.9 | ndcg20: 0.0752465847509586 | ndcg20/std: 0.0075593063303510755\n",
      "* Epoch (2/100) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3/100 * Epoch (train): 100%|| 24/24 [00:27<00:00,  1.14s/it, _timer/_fps=186.947, _timer/batch_time=0.813, _timer/data_time=0.446, _timer/model_time=0.367, loss=7.348, lr=0.010, map10=0.000e+00, momentum=0.900, ndcg20=0.000e+00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (3/100) loss: 7.420824758895975 | lr: 0.01 | map10: 0.0 | map10/std: 0.0 | momentum: 0.9 | ndcg20: 0.0 | ndcg20/std: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3/100 * Epoch (valid): 100%|| 24/24 [00:13<00:00,  1.82it/s, _timer/_fps=434.267, _timer/batch_time=0.350, _timer/data_time=0.022, _timer/model_time=0.328, loss=7.318, lr=0.010, map10=0.118, momentum=0.900, ndcg20=0.067]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid (3/100) loss: 7.394668850046122 | lr: 0.01 | map10: 0.13780646224487694 | map10/std: 0.017070203664227363 | momentum: 0.9 | ndcg20: 0.07514734006678031 | ndcg20/std: 0.007966531944900112\n",
      "* Epoch (3/100) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4/100 * Epoch (train): 100%|| 24/24 [00:27<00:00,  1.16s/it, _timer/_fps=169.459, _timer/batch_time=0.897, _timer/data_time=0.524, _timer/model_time=0.373, loss=7.342, lr=0.010, map10=0.000e+00, momentum=0.900, ndcg20=0.000e+00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (4/100) loss: 7.420100878406045 | lr: 0.01 | map10: 0.0 | map10/std: 0.0 | momentum: 0.9 | ndcg20: 0.0 | ndcg20/std: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4/100 * Epoch (valid): 100%|| 24/24 [00:13<00:00,  1.78it/s, _timer/_fps=334.189, _timer/batch_time=0.455, _timer/data_time=0.024, _timer/model_time=0.431, loss=7.309, lr=0.010, map10=0.132, momentum=0.900, ndcg20=0.072]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid (4/100) loss: 7.371081373075775 | lr: 0.01 | map10: 0.14047056955612258 | map10/std: 0.01900748433859079 | momentum: 0.9 | ndcg20: 0.07832785569476766 | ndcg20/std: 0.008021938772896852\n",
      "* Epoch (4/100) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5/100 * Epoch (train): 100%|| 24/24 [00:29<00:00,  1.24s/it, _timer/_fps=173.845, _timer/batch_time=0.874, _timer/data_time=0.496, _timer/model_time=0.378, loss=7.368, lr=0.010, map10=0.000e+00, momentum=0.900, ndcg20=0.000e+00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (5/100) loss: 7.394936804739846 | lr: 0.01 | map10: 0.0 | map10/std: 0.0 | momentum: 0.9 | ndcg20: 0.0 | ndcg20/std: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5/100 * Epoch (valid): 100%|| 24/24 [00:13<00:00,  1.75it/s, _timer/_fps=442.243, _timer/batch_time=0.344, _timer/data_time=0.020, _timer/model_time=0.323, loss=7.348, lr=0.010, map10=0.131, momentum=0.900, ndcg20=0.068]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid (5/100) loss: 7.356114329407547 | lr: 0.01 | map10: 0.1418781919392529 | map10/std: 0.01483220975141328 | momentum: 0.9 | ndcg20: 0.07928699738537237 | ndcg20/std: 0.007633160690774158\n",
      "* Epoch (5/100) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6/100 * Epoch (train): 100%|| 24/24 [00:26<00:00,  1.10s/it, _timer/_fps=192.517, _timer/batch_time=0.790, _timer/data_time=0.420, _timer/model_time=0.369, loss=7.359, lr=0.010, map10=0.000e+00, momentum=0.900, ndcg20=0.000e+00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (6/100) loss: 7.395024589513311 | lr: 0.01 | map10: 0.0 | map10/std: 0.0 | momentum: 0.9 | ndcg20: 0.0 | ndcg20/std: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6/100 * Epoch (valid): 100%|| 24/24 [00:13<00:00,  1.83it/s, _timer/_fps=440.737, _timer/batch_time=0.345, _timer/data_time=0.023, _timer/model_time=0.322, loss=7.342, lr=0.010, map10=0.106, momentum=0.900, ndcg20=0.067]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid (6/100) loss: 7.366093260405079 | lr: 0.01 | map10: 0.13294146644161237 | map10/std: 0.018435071658037913 | momentum: 0.9 | ndcg20: 0.07597670783072905 | ndcg20/std: 0.007776523998895446\n",
      "* Epoch (6/100) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7/100 * Epoch (train): 100%|| 24/24 [00:27<00:00,  1.15s/it, _timer/_fps=170.666, _timer/batch_time=0.891, _timer/data_time=0.520, _timer/model_time=0.371, loss=7.372, lr=0.010, map10=0.000e+00, momentum=0.900, ndcg20=0.000e+00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (7/100) loss: 7.3994960949120925 | lr: 0.01 | map10: 0.0 | map10/std: 0.0 | momentum: 0.9 | ndcg20: 0.0 | ndcg20/std: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7/100 * Epoch (valid): 100%|| 24/24 [00:14<00:00,  1.66it/s, _timer/_fps=418.413, _timer/batch_time=0.363, _timer/data_time=0.023, _timer/model_time=0.340, loss=7.358, lr=0.010, map10=0.117, momentum=0.900, ndcg20=0.072]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid (7/100) loss: 7.36685700321829 | lr: 0.01 | map10: 0.1315575515000236 | map10/std: 0.01896158695151225 | momentum: 0.9 | ndcg20: 0.0753728217340463 | ndcg20/std: 0.00830497352161689\n",
      "* Epoch (7/100) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8/100 * Epoch (train): 100%|| 24/24 [00:26<00:00,  1.12s/it, _timer/_fps=198.655, _timer/batch_time=0.765, _timer/data_time=0.396, _timer/model_time=0.369, loss=7.363, lr=0.010, map10=0.000e+00, momentum=0.900, ndcg20=0.000e+00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (8/100) loss: 7.390039569652633 | lr: 0.01 | map10: 0.0 | map10/std: 0.0 | momentum: 0.9 | ndcg20: 0.0 | ndcg20/std: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8/100 * Epoch (valid): 100%|| 24/24 [00:13<00:00,  1.80it/s, _timer/_fps=423.844, _timer/batch_time=0.359, _timer/data_time=0.021, _timer/model_time=0.338, loss=7.347, lr=0.010, map10=0.114, momentum=0.900, ndcg20=0.070]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid (8/100) loss: 7.357703402184493 | lr: 0.01 | map10: 0.13947743097480556 | map10/std: 0.017390441198534386 | momentum: 0.9 | ndcg20: 0.07939761872125779 | ndcg20/std: 0.007953575870584607\n",
      "* Epoch (8/100) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9/100 * Epoch (train): 100%|| 24/24 [00:26<00:00,  1.11s/it, _timer/_fps=189.733, _timer/batch_time=0.801, _timer/data_time=0.437, _timer/model_time=0.364, loss=7.349, lr=0.010, map10=0.000e+00, momentum=0.900, ndcg20=0.000e+00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (9/100) loss: 7.393237271845735 | lr: 0.01 | map10: 0.0 | map10/std: 0.0 | momentum: 0.9 | ndcg20: 0.0 | ndcg20/std: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9/100 * Epoch (valid): 100%|| 24/24 [00:12<00:00,  1.86it/s, _timer/_fps=467.186, _timer/batch_time=0.325, _timer/data_time=0.021, _timer/model_time=0.304, loss=7.330, lr=0.010, map10=0.124, momentum=0.900, ndcg20=0.066]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid (9/100) loss: 7.363185141102368 | lr: 0.01 | map10: 0.13569314905152413 | map10/std: 0.0166163718338287 | momentum: 0.9 | ndcg20: 0.07895752016874338 | ndcg20/std: 0.007762595069919511\n",
      "* Epoch (9/100) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/100 * Epoch (train): 100%|| 24/24 [00:26<00:00,  1.09s/it, _timer/_fps=179.271, _timer/batch_time=0.848, _timer/data_time=0.488, _timer/model_time=0.360, loss=7.339, lr=0.010, map10=0.000e+00, momentum=0.900, ndcg20=0.000e+00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (10/100) loss: 7.390272671655314 | lr: 0.01 | map10: 0.0 | map10/std: 0.0 | momentum: 0.9 | ndcg20: 0.0 | ndcg20/std: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/100 * Epoch (valid): 100%|| 24/24 [00:12<00:00,  1.86it/s, _timer/_fps=456.896, _timer/batch_time=0.333, _timer/data_time=0.019, _timer/model_time=0.314, loss=7.316, lr=0.010, map10=0.130, momentum=0.900, ndcg20=0.072]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid (10/100) loss: 7.359320659511135 | lr: 0.01 | map10: 0.14164599831530592 | map10/std: 0.017763887164391423 | momentum: 0.9 | ndcg20: 0.0795116835872069 | ndcg20/std: 0.007699242698728413\n",
      "* Epoch (10/100) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = ContextBERT4Rec(n_items=len(item2idx)+1, mask_ratio=0.2, hidden_size=128)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "lr_scheduler = StepLR(optimizer, step_size=20, gamma=0.1)\n",
    "engine = dl.DeviceEngine('cpu')\n",
    "hparams = {\n",
    "    \"anneal_cap\": 0.2,\n",
    "    \"total_anneal_steps\": 6000,\n",
    "}\n",
    "\n",
    "\n",
    "callbacks = [\n",
    "    dl.NDCGCallback(\"logits\", \"targets\", [20]),\n",
    "    dl.MAPCallback(\"logits\", \"targets\", [10]),\n",
    "    dl.OptimizerCallback(\"loss\", accumulation_steps=1),\n",
    "    dl.EarlyStoppingCallback(\n",
    "        patience=5, loader_key=\"valid\", metric_key=\"map10\", minimize=False\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "runner = RecSysRunner()\n",
    "runner.train(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    engine=engine,\n",
    "    hparams=hparams,\n",
    "    scheduler=lr_scheduler,\n",
    "    loaders=loaders,\n",
    "    num_epochs=100,\n",
    "    verbose=True,\n",
    "    timeit=True,\n",
    "    callbacks=callbacks,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3792dee0-76e9-46b0-aa33-28a8af1cb782",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [00:06, 15.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6040\n"
     ]
    }
   ],
   "source": [
    "test_dataset = MyDataset(ds=joined, num_items=n_items, phase='test',item2idx=item2idx)\n",
    "\n",
    "\n",
    "inference_loader = DataLoader(test_dataset, \n",
    "                              batch_size=joined.shape[0]//100, \n",
    "                              collate_fn=collate_fn_train,)\n",
    "\n",
    "preds = []\n",
    "\n",
    "for prediction in tqdm(runner.predict_loader(loader=inference_loader)):\n",
    "    preds.extend(prediction.detach().cpu().numpy().tolist())\n",
    "    \n",
    "print(len(preds))\n",
    "assert len(preds) == joined.shape[0]\n",
    "\n",
    "joined['preds_contextbert4rec'] = preds\n",
    "joined['recs_contextbert4rec_10'] = joined['preds_contextbert4rec'].apply(lambda x: np.argsort(-np.array(x))[:10])\n",
    "joined['recs_contextbert4rec_10'] = joined['recs_contextbert4rec_10'].apply(lambda x: [idx2item[t-1] for t in x])\n",
    "joined['recs_contextbert4rec_5'] = joined['preds_contextbert4rec'].apply(lambda x: np.argsort(-np.array(x))[:5])\n",
    "joined['recs_contextbert4rec_5'] = joined['recs_contextbert4rec_5'].apply(lambda x: [idx2item[t-1] for t in x])\n",
    "joined.drop(['preds_contextbert4rec'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "19b4f466-7ea6-4b21-a30f-117462516829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ndcg': 0.15325097636829188, 'recall': 0.03236868958740314}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_recommender(joined, model_preds='recs_contextbert4rec_10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e3b42d47-b70c-4f2a-823e-a946b6ea8b45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ndcg': 0.0935407587039434, 'recall': 0.017191631300404735}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_recommender(joined, model_preds='recs_contextbert4rec_5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5153c08b-6353-4ac9-910c-c83b9622c914",
   "metadata": {},
   "source": [
    "### Full context (hours + day of the week + date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6805013f-5c16-4a05-84b9-30622d96050f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>weekday</th>\n",
       "      <th>hour</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>2000-12-31 22:12:40</td>\n",
       "      <td>6</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "      <td>2000-12-31 22:35:09</td>\n",
       "      <td>6</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "      <td>2000-12-31 22:32:48</td>\n",
       "      <td>6</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "      <td>2000-12-31 22:04:35</td>\n",
       "      <td>6</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "      <td>2001-01-06 23:38:11</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating           timestamp  weekday  hour  date\n",
       "0        1     1193       5 2000-12-31 22:12:40        6    22     0\n",
       "1        1      661       3 2000-12-31 22:35:09        6    22     0\n",
       "2        1      914       3 2000-12-31 22:32:48        6    22     0\n",
       "3        1     3408       4 2000-12-31 22:04:35        6    22     0\n",
       "4        1     2355       5 2001-01-06 23:38:11        5    23     1"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['date'] = pd.to_datetime(df['timestamp'].dt.date).astype('int64')\n",
    "map_dates = {k: i for i, k in enumerate(df.date.unique())}\n",
    "df['date'] = df['date'].map(map_dates)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "59bad503-a27b-4f2c-9657-8d186d152d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dates = df.date.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d56e93ed-dc47-4056-bef7-294f489b1cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RandomSplit(test_fraction=0.2)\n",
    "train_df, valid_df, test_df = splitter(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "859f12e0-5c12-4db4-ade0-100ae4cd1812",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_grouped = train_df.groupby('user_id').apply(\n",
    "    lambda x: [(t1, t2, t3, t4, t5) for t1, t2, t3, t4, t5 in sorted(zip(x.item_id, \n",
    "                                                                 x.timestamp,\n",
    "                                                                 x.date,\n",
    "                                                                 x.weekday,\n",
    "                                                                 x.hour), key=lambda x: x[1])]\n",
    ").reset_index()\n",
    "train_grouped.rename({0:'train_interactions'}, axis=1, inplace=True)\n",
    "\n",
    "valid_grouped = valid_df.groupby('user_id').apply(\n",
    "    lambda x: [(t1, t2, t3, t4, t5) for t1, t2, t3, t4, t5 in sorted(zip(x.item_id,\n",
    "                                                         x.timestamp,\n",
    "                                                         x.date,\n",
    "                                                         x.weekday,\n",
    "                                                         x.hour), key=lambda x: x[1])]\n",
    ").reset_index()\n",
    "valid_grouped.rename({0:'valid_interactions'}, axis=1, inplace=True)\n",
    "\n",
    "test_grouped = test_df.groupby('user_id').apply(\n",
    "    lambda x: [(t1, t2, t3, t4, t5) for t1, t2, t3, t4, t5 in sorted(zip(x.item_id,\n",
    "                                                         x.timestamp,\n",
    "                                                         x.date,\n",
    "                                                         x.weekday,\n",
    "                                                         x.hour), key=lambda x: x[1])]\n",
    ").reset_index()\n",
    "test_grouped.rename({0:'test_interactions'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "13f8bb2c-678c-4d9c-8f7d-f0fd620d084a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>train_interactions</th>\n",
       "      <th>valid_interactions</th>\n",
       "      <th>test_interactions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[(3186, 2000-12-31 22:00:19, 0, 6, 22), (1270,...</td>\n",
       "      <td>[(2791, 2000-12-31 22:36:28, 0, 6, 22), (2321,...</td>\n",
       "      <td>[(2687, 2001-01-06 23:37:48, 1, 5, 23), (745, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[(1198, 2000-12-31 21:28:44, 0, 6, 21), (1210,...</td>\n",
       "      <td>[(2028, 2000-12-31 21:56:13, 0, 6, 21), (2571,...</td>\n",
       "      <td>[(1372, 2000-12-31 21:59:01, 0, 6, 21), (1552,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[(593, 2000-12-31 21:10:18, 0, 6, 21), (2858, ...</td>\n",
       "      <td>[(648, 2000-12-31 21:24:27, 0, 6, 21), (2735, ...</td>\n",
       "      <td>[(1270, 2000-12-31 21:30:31, 0, 6, 21), (1079,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[(1210, 2000-12-31 20:18:44, 0, 6, 20), (1097,...</td>\n",
       "      <td>[(2947, 2000-12-31 20:23:50, 0, 6, 20), (1214,...</td>\n",
       "      <td>[(1240, 2000-12-31 20:24:20, 0, 6, 20), (2951,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>[(2717, 2000-12-31 05:37:52, 0, 6, 5), (908, 2...</td>\n",
       "      <td>[(2323, 2000-12-31 06:50:45, 0, 6, 6), (272, 2...</td>\n",
       "      <td>[(1715, 2000-12-31 06:58:11, 0, 6, 6), (1653, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                                 train_interactions  \\\n",
       "0        1  [(3186, 2000-12-31 22:00:19, 0, 6, 22), (1270,...   \n",
       "1        2  [(1198, 2000-12-31 21:28:44, 0, 6, 21), (1210,...   \n",
       "2        3  [(593, 2000-12-31 21:10:18, 0, 6, 21), (2858, ...   \n",
       "3        4  [(1210, 2000-12-31 20:18:44, 0, 6, 20), (1097,...   \n",
       "4        5  [(2717, 2000-12-31 05:37:52, 0, 6, 5), (908, 2...   \n",
       "\n",
       "                                  valid_interactions  \\\n",
       "0  [(2791, 2000-12-31 22:36:28, 0, 6, 22), (2321,...   \n",
       "1  [(2028, 2000-12-31 21:56:13, 0, 6, 21), (2571,...   \n",
       "2  [(648, 2000-12-31 21:24:27, 0, 6, 21), (2735, ...   \n",
       "3  [(2947, 2000-12-31 20:23:50, 0, 6, 20), (1214,...   \n",
       "4  [(2323, 2000-12-31 06:50:45, 0, 6, 6), (272, 2...   \n",
       "\n",
       "                                   test_interactions  \n",
       "0  [(2687, 2001-01-06 23:37:48, 1, 5, 23), (745, ...  \n",
       "1  [(1372, 2000-12-31 21:59:01, 0, 6, 21), (1552,...  \n",
       "2  [(1270, 2000-12-31 21:30:31, 0, 6, 21), (1079,...  \n",
       "3  [(1240, 2000-12-31 20:24:20, 0, 6, 20), (2951,...  \n",
       "4  [(1715, 2000-12-31 06:58:11, 0, 6, 6), (1653, ...  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined = train_grouped.merge(valid_grouped).merge(test_grouped)\n",
    "joined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "23c03e96-62d0-49dc-9035-42ab2e5c5342",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, ds, num_items, item2idx, phase='valid', N=200):\n",
    "        super().__init__()\n",
    "        self.ds = ds\n",
    "        self.phase = phase\n",
    "        self.n_items = num_items\n",
    "        self.item2idx = item2idx\n",
    "        self.N = N \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        row = self.ds.iloc[idx]\n",
    "        \n",
    "        x_input = np.zeros(self.n_items+1)\n",
    "        x_input[[self.item2idx[x[0]]+1 for x in row['train_interactions'] if x[0] in self.item2idx]] = 1\n",
    "        \n",
    "        date = [x[2] for x in row['train_interactions'] if x[0] in self.item2idx][-self.N+1:]\n",
    "        days_of_weeks = [x[3] for x in row['train_interactions'] if x[0] in self.item2idx][-self.N+1:]\n",
    "        hours = [x[4] for x in row['train_interactions'] if x[0] in self.item2idx][-self.N+1:]\n",
    "        \n",
    "        seq_input = [self.item2idx[x[0]]+1 for x in row['train_interactions'] if x[0] in self.item2idx][-self.N+1:]\n",
    "        \n",
    "        targets = np.zeros(self.n_items+1)\n",
    "        \n",
    "        date_valid = row['valid_interactions'][0][2]\n",
    "        date_test = row['test_interactions'][0][2]\n",
    "        \n",
    "        dow_valid = row['valid_interactions'][0][3]\n",
    "        dow_test = row['test_interactions'][0][3]\n",
    "        \n",
    "        hours_valid = row['valid_interactions'][0][4]\n",
    "        hours_test = row['test_interactions'][0][4]\n",
    "        \n",
    "        if self.phase == 'train':\n",
    "            return (seq_input, date, days_of_weeks, hours, date_valid, dow_valid, hours_valid)\n",
    "        elif self.phase == 'valid':\n",
    "            targets[[self.item2idx[x[0]]+1 for x in row['valid_interactions'] if x[0] in self.item2idx]] = 1\n",
    "        else:\n",
    "            return (seq_input, date, days_of_weeks, hours, date_test, dow_test, hours_test)\n",
    "            \n",
    "        return (targets, seq_input, date, days_of_weeks, hours, date_valid, dow_valid, hours_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "13616e1c-c964-45d0-970e-479427a236a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6040 6040\n"
     ]
    }
   ],
   "source": [
    "n_items = len(item2idx)\n",
    "\n",
    "train = MyDataset(ds=joined,\n",
    "                  num_items=n_items, \n",
    "                  item2idx=item2idx,\n",
    "                  phase='train')\n",
    "\n",
    "valid = MyDataset(ds=joined,\n",
    "                  num_items=n_items,\n",
    "                  item2idx=item2idx,\n",
    "                  phase='valid')\n",
    "\n",
    "print(len(train),len(valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a3fe43f7-de07-4d3e-9ece-3b044aa1050c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_train(batch: List[Tuple[torch.Tensor]]) -> Dict[str, torch.Tensor]: \n",
    "    \n",
    "    seq_i,date, days_of_weeks,hours,date_valid,dow_valid,hours_valid = zip(*batch)\n",
    "    \n",
    "    seq_len = torch.Tensor([len(x) for x in seq_i])\n",
    "    date_valid = torch.Tensor([x for x in date_valid])\n",
    "    dow_valid = torch.Tensor([x for x in dow_valid])\n",
    "    hours_valid = torch.Tensor([x for x in hours_valid])\n",
    "    seq_i = pad_sequence([torch.Tensor(t) for t in seq_i]).T    \n",
    "    days_of_weeks = pad_sequence([torch.Tensor(t) for t in days_of_weeks]).T\n",
    "    hours = pad_sequence([torch.Tensor(t) for t in hours]).T\n",
    "    date = pad_sequence([torch.Tensor(t) for t in date]).T\n",
    "    \n",
    "    return {'seq_i': seq_i, \n",
    "            'seq_len':seq_len,\n",
    "            'date': date,\n",
    "            'dow': days_of_weeks,\n",
    "            'hours': hours,\n",
    "            'date_valid': date_valid,\n",
    "            'dow_valid': dow_valid,\n",
    "            'hours_valid': hours_valid}\n",
    "\n",
    "\n",
    "def collate_fn_valid(batch: List[Tuple[torch.Tensor]]) -> Dict[str, torch.Tensor]:\n",
    "    \n",
    "    y, seq_i, date, days_of_weeks, hours, date_valid, dow_valid, hours_valid = zip(*batch)\n",
    "    \n",
    "    seq_len = torch.Tensor([len(x) for x in seq_i]).long()\n",
    "    seq_i = pad_sequence([torch.Tensor(t) for t in seq_i]).T.long()\n",
    "    date = pad_sequence([torch.Tensor(t) for t in date]).T.long()\n",
    "    days_of_weeks = pad_sequence([torch.Tensor(t) for t in days_of_weeks]).T.long()\n",
    "    hours = pad_sequence([torch.Tensor(t) for t in hours]).T.long()\n",
    "    date_valid = torch.Tensor([x for x in date_valid])\n",
    "    dow_valid = torch.Tensor([x for x in dow_valid])\n",
    "    hours_valid = torch.Tensor([x for x in hours_valid])\n",
    "            \n",
    "    targets = pad_sequence([torch.Tensor(t) for t in y]).T\n",
    "\n",
    "    return {\"targets\": targets,\n",
    "            'seq_i': seq_i,\n",
    "            'seq_len':seq_len,\n",
    "            'date': date,\n",
    "            'dow': days_of_weeks,\n",
    "            'hours': hours,\n",
    "            'date_valid': date_valid,\n",
    "            'dow_valid': dow_valid,\n",
    "            'hours_valid': hours_valid}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4ce4e0e5-8431-438e-877b-a5071aac126d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ContextBERT4Rec(BERT4Rec):\n",
    "\n",
    "    def __init__(self, n_items, n_dates, hidden_size, mask_ratio):\n",
    "        super(BERT4Rec, self).__init__()\n",
    "        \n",
    "        self.n_layers = 2\n",
    "        self.n_heads = 2\n",
    "        self.hidden_size = hidden_size  \n",
    "        self.inner_size = 128 \n",
    "        self.hidden_dropout_prob = 0.2\n",
    "        self.attn_dropout_prob = 0.2\n",
    "        self.hidden_act = 'sigmoid'\n",
    "        self.layer_norm_eps = 1e-5\n",
    "        self.ITEM_SEQ = 'seq_i'\n",
    "        self.ITEM_SEQ_LEN = 'seq_len'\n",
    "        self.max_seq_length = 200\n",
    "        \n",
    "\n",
    "        self.mask_ratio = mask_ratio\n",
    "\n",
    "        self.loss_type =  'CE'\n",
    "        self.initializer_range = 1e-2\n",
    "\n",
    "        # load dataset info\n",
    "        self.n_items = n_items\n",
    "        self.n_dates = n_dates\n",
    "        self.mask_token = self.n_items\n",
    "        self.mask_item_length = int(self.mask_ratio * self.max_seq_length)\n",
    "\n",
    "        # define layers and loss\n",
    "        self.weekday_embedding = nn.Embedding(7, self.hidden_size)\n",
    "        self.hours_embedding = nn.Embedding(24, self.hidden_size)\n",
    "        self.item_embedding = nn.Embedding(self.n_items + 1, self.hidden_size, padding_idx=0)  \n",
    "        self.date_embedding = nn.Embedding(self.n_dates + 1, self.hidden_size)\n",
    "        self.position_embedding = nn.Embedding(self.max_seq_length + 1, self.hidden_size)  \n",
    "        self.trm_encoder = TransformerEncoder(\n",
    "            n_layers=self.n_layers,\n",
    "            n_heads=self.n_heads,\n",
    "            hidden_size=self.hidden_size,\n",
    "            inner_size=self.inner_size,\n",
    "            hidden_dropout_prob=self.hidden_dropout_prob,\n",
    "            attn_dropout_prob=self.attn_dropout_prob,\n",
    "            hidden_act=self.hidden_act,\n",
    "            layer_norm_eps=self.layer_norm_eps\n",
    "        )\n",
    "\n",
    "        self.LayerNorm = nn.LayerNorm(self.hidden_size, eps=self.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(self.hidden_dropout_prob)\n",
    "\n",
    "        try:\n",
    "            assert self.loss_type in ['BPR', 'CE']\n",
    "        except AssertionError:\n",
    "            raise AssertionError(\"Make sure 'loss_type' in ['BPR', 'CE']!\")\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "\n",
    "\n",
    "    def reconstruct_test_data(self,\n",
    "                              item_seq,\n",
    "                              item_seq_len,\n",
    "                              date,\n",
    "                              dow,\n",
    "                              hours,\n",
    "                              date_valid,\n",
    "                              dow_valid,\n",
    "                              hours_valid,\n",
    "                              particular_day=-1,\n",
    "                              ):\n",
    "        \"\"\"\n",
    "        Add mask token at the last position according to the lengths of item_seq\n",
    "        \"\"\"\n",
    "        padding = torch.zeros(item_seq.size(0), dtype=torch.long, device=item_seq.device)  # [B]\n",
    "        item_seq = torch.cat((item_seq, padding.unsqueeze(-1)), dim=-1)  # [B max_len+1]\n",
    "        date = torch.cat((date, padding.unsqueeze(-1)), dim=-1)\n",
    "        dow = torch.cat((dow, padding.unsqueeze(-1)), dim=-1)\n",
    "        hours = torch.cat((hours, padding.unsqueeze(-1)), dim=-1)\n",
    "        for batch_id, last_position in enumerate(item_seq_len):\n",
    "            item_seq[batch_id][last_position] = self.mask_token\n",
    "            if particular_day == -1:\n",
    "                date[batch_id][last_position] = date_valid[batch_id]\n",
    "                dow[batch_id][last_position] = dow_valid[batch_id]\n",
    "                hours[batch_id][last_position] = hours_valid[batch_id]\n",
    "            else:\n",
    "                date[batch_id][last_position] = particular_day\n",
    "                dow[batch_id][last_position] = particular_day\n",
    "                hours[batch_id][last_position] = particular_day\n",
    "        return item_seq, date, dow, hours\n",
    "\n",
    "    def forward(self, item_seq, date, dow, hours, return_explanations=False):\n",
    "        \n",
    "        \n",
    "        dow_embeddings = self.weekday_embedding(dow.long())\n",
    "        hours_embeddings = self.hours_embedding(hours.long())\n",
    "        date_embeddings = self.date_embedding(date.long())\n",
    "        \n",
    "        position_ids = torch.arange(item_seq.size(1), dtype=torch.long, device=item_seq.device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand_as(item_seq)\n",
    "        position_embedding = self.position_embedding(position_ids)\n",
    "        item_emb = self.item_embedding(item_seq)\n",
    "        input_emb = item_emb + position_embedding + dow_embeddings + hours_embeddings + date_embeddings\n",
    "        input_emb = self.LayerNorm(input_emb)\n",
    "        input_emb = self.dropout(input_emb)\n",
    "        extended_attention_mask = self.get_attention_mask(item_seq)\n",
    "        if return_explanations:\n",
    "            trm_output, explanations = self.trm_encoder(input_emb, extended_attention_mask, output_all_encoded_layers=True,\n",
    "                                         return_explanations=return_explanations)\n",
    "        else:\n",
    "            trm_output = self.trm_encoder(input_emb, extended_attention_mask, output_all_encoded_layers=True,\n",
    "                                         return_explanations=return_explanations)\n",
    "            \n",
    "        output = trm_output[-1]\n",
    "        \n",
    "        if return_explanations:\n",
    "            return output, explanations\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "\n",
    "    def calculate_loss(self, interaction):\n",
    "        item_seq = interaction[self.ITEM_SEQ].long()\n",
    "        masked_item_seq, pos_items, neg_items, masked_index = self.reconstruct_train_data(item_seq)\n",
    "\n",
    "        seq_output = self.forward(masked_item_seq, date=interaction['date'], dow=interaction['dow'], hours=interaction['hours'])\n",
    "        pred_index_map = self.multi_hot_embed(masked_index, masked_item_seq.size(-1))  \n",
    "        pred_index_map = pred_index_map.view(masked_index.size(0), masked_index.size(1), -1)  \n",
    "        seq_output = torch.bmm(pred_index_map, seq_output)  \n",
    "\n",
    "        if self.loss_type == 'BPR':\n",
    "            pos_items_emb = self.item_embedding(pos_items)  \n",
    "            neg_items_emb = self.item_embedding(neg_items)  \n",
    "            pos_score = torch.sum(seq_output * pos_items_emb, dim=-1) \n",
    "            neg_score = torch.sum(seq_output * neg_items_emb, dim=-1)  \n",
    "            targets = (masked_index > 0).float()\n",
    "            loss = - torch.sum(torch.log(1e-14 + torch.sigmoid(pos_score - neg_score)) * targets) \\\n",
    "                   / torch.sum(targets)\n",
    "            return loss\n",
    "\n",
    "        elif self.loss_type == 'CE':\n",
    "            loss_fct = nn.CrossEntropyLoss(reduction='none')\n",
    "            test_item_emb = self.item_embedding.weight[:self.n_items]  \n",
    "            logits = torch.matmul(seq_output, test_item_emb.transpose(0, 1))  \n",
    "            targets = (masked_index > 0).float().view(-1) \n",
    "\n",
    "            loss = torch.sum(loss_fct(logits.view(-1, test_item_emb.size(0)), pos_items.view(-1)) * targets) \\\n",
    "                   / torch.sum(targets)\n",
    "            return loss\n",
    "        else:\n",
    "            raise NotImplementedError(\"Make sure 'loss_type' in ['BPR', 'CE']!\")\n",
    "\n",
    "\n",
    "    def full_sort_predict(self, \n",
    "                          interaction,\n",
    "                          return_explanations=False,\n",
    "                          particular_day=-1):\n",
    "        \n",
    "        item_seq = interaction[self.ITEM_SEQ].long()\n",
    "        item_seq_len = interaction[self.ITEM_SEQ_LEN].long()\n",
    "        item_seq, date, dow, hours = self.reconstruct_test_data(item_seq,\n",
    "                                              item_seq_len,\n",
    "                                              date=interaction['date'],\n",
    "                                              dow=interaction['dow'],\n",
    "                                              hours=interaction['hours'],\n",
    "                                              date_valid=interaction['date_valid'].long(),\n",
    "                                              dow_valid=interaction['dow_valid'].long(),\n",
    "                                              hours_valid=interaction['hours_valid'].long(),\n",
    "                                              particular_day=particular_day)\n",
    "        \n",
    "        \n",
    "        if return_explanations:\n",
    "            seq_output, expl = self.forward(item_seq,\n",
    "                                            date=date,\n",
    "                                            dow=dow,\n",
    "                                            hours=hours,\n",
    "                                            return_explanations=return_explanations)\n",
    "        else:\n",
    "            seq_output = self.forward(item_seq,\n",
    "                                      date=date,\n",
    "                                      dow=dow,\n",
    "                                      hours=hours,\n",
    "                                      return_explanations=return_explanations)\n",
    "            \n",
    "        \n",
    "        seq_output = self.gather_indexes(seq_output, item_seq_len - 1) \n",
    "        test_items_emb = self.item_embedding.weight[:self.n_items]  \n",
    "        scores = torch.matmul(seq_output, test_items_emb.transpose(0, 1))  \n",
    "                \n",
    "        idxs = item_seq.nonzero()\n",
    "        item_seq[item_seq==self.n_items] = 0\n",
    "        scores[idxs[:,0], item_seq[idxs[:,0],idxs[:,1]].long()] = -1000\n",
    "\n",
    "        if return_explanations:\n",
    "            return scores, expl\n",
    "        else:\n",
    "            return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "46b1c582-9dfe-4bf3-9aa4-1124d30797f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = {\n",
    "        \"train\": DataLoader(train, batch_size=256, collate_fn=collate_fn_train),\n",
    "        \"valid\": DataLoader(valid, batch_size=256, collate_fn=collate_fn_valid),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "30bb099e-1e0e-40f7-99cf-8355324b2c5d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/100 * Epoch (train): 100%|| 24/24 [00:27<00:00,  1.16s/it, _timer/_fps=173.094, _timer/batch_time=0.878, _timer/data_time=0.519, _timer/model_time=0.359, loss=7.394, lr=0.010, map10=0.000e+00, momentum=0.900, ndcg20=0.000e+00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (1/100) loss: 7.536861446835347 | lr: 0.01 | map10: 0.0 | map10/std: 0.0 | momentum: 0.9 | ndcg20: 0.0 | ndcg20/std: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/100 * Epoch (valid): 100%|| 24/24 [00:13<00:00,  1.78it/s, _timer/_fps=354.476, _timer/batch_time=0.429, _timer/data_time=0.026, _timer/model_time=0.403, loss=7.323, lr=0.010, map10=0.102, momentum=0.900, ndcg20=0.059]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid (1/100) loss: 7.427616329066801 | lr: 0.01 | map10: 0.12805838611544365 | map10/std: 0.018024661106085162 | momentum: 0.9 | ndcg20: 0.06925669961812482 | ndcg20/std: 0.007266366313612276\n",
      "* Epoch (1/100) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2/100 * Epoch (train): 100%|| 24/24 [00:31<00:00,  1.32s/it, _timer/_fps=178.983, _timer/batch_time=0.849, _timer/data_time=0.486, _timer/model_time=0.363, loss=7.345, lr=0.010, map10=0.000e+00, momentum=0.900, ndcg20=0.000e+00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (2/100) loss: 7.444541586945389 | lr: 0.01 | map10: 0.0 | map10/std: 0.0 | momentum: 0.9 | ndcg20: 0.0 | ndcg20/std: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2/100 * Epoch (valid): 100%|| 24/24 [00:12<00:00,  1.87it/s, _timer/_fps=449.461, _timer/batch_time=0.338, _timer/data_time=0.021, _timer/model_time=0.318, loss=7.319, lr=0.010, map10=0.109, momentum=0.900, ndcg20=0.062]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid (2/100) loss: 7.41803659919082 | lr: 0.01 | map10: 0.13242110419549685 | map10/std: 0.016910505083480264 | momentum: 0.9 | ndcg20: 0.07515916410364853 | ndcg20/std: 0.007424306428190924\n",
      "* Epoch (2/100) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3/100 * Epoch (train): 100%|| 24/24 [00:26<00:00,  1.10s/it, _timer/_fps=168.175, _timer/batch_time=0.904, _timer/data_time=0.532, _timer/model_time=0.372, loss=7.364, lr=0.010, map10=0.000e+00, momentum=0.900, ndcg20=0.000e+00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (3/100) loss: 7.4214393716774225 | lr: 0.01 | map10: 0.0 | map10/std: 0.0 | momentum: 0.9 | ndcg20: 0.0 | ndcg20/std: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3/100 * Epoch (valid): 100%|| 24/24 [00:12<00:00,  1.88it/s, _timer/_fps=445.437, _timer/batch_time=0.341, _timer/data_time=0.021, _timer/model_time=0.320, loss=7.335, lr=0.010, map10=0.101, momentum=0.900, ndcg20=0.064]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid (3/100) loss: 7.388891737982138 | lr: 0.01 | map10: 0.13336395080515878 | map10/std: 0.017215231857943103 | momentum: 0.9 | ndcg20: 0.07408513371518118 | ndcg20/std: 0.007605365468550447\n",
      "* Epoch (3/100) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4/100 * Epoch (train): 100%|| 24/24 [00:25<00:00,  1.06s/it, _timer/_fps=185.563, _timer/batch_time=0.819, _timer/data_time=0.458, _timer/model_time=0.361, loss=7.334, lr=0.010, map10=0.000e+00, momentum=0.900, ndcg20=0.000e+00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (4/100) loss: 7.4067662971698685 | lr: 0.01 | map10: 0.0 | map10/std: 0.0 | momentum: 0.9 | ndcg20: 0.0 | ndcg20/std: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4/100 * Epoch (valid): 100%|| 24/24 [00:12<00:00,  1.90it/s, _timer/_fps=427.828, _timer/batch_time=0.355, _timer/data_time=0.021, _timer/model_time=0.334, loss=7.346, lr=0.010, map10=0.117, momentum=0.900, ndcg20=0.067]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid (4/100) loss: 7.400157096685952 | lr: 0.01 | map10: 0.1368013177585128 | map10/std: 0.016712762275400046 | momentum: 0.9 | ndcg20: 0.07734376441761358 | ndcg20/std: 0.00803253663960003\n",
      "* Epoch (4/100) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5/100 * Epoch (train): 100%|| 24/24 [00:26<00:00,  1.09s/it, _timer/_fps=174.452, _timer/batch_time=0.871, _timer/data_time=0.501, _timer/model_time=0.370, loss=7.339, lr=0.010, map10=0.000e+00, momentum=0.900, ndcg20=0.000e+00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (5/100) loss: 7.378267670940881 | lr: 0.01 | map10: 0.0 | map10/std: 0.0 | momentum: 0.9 | ndcg20: 0.0 | ndcg20/std: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5/100 * Epoch (valid): 100%|| 24/24 [00:12<00:00,  1.88it/s, _timer/_fps=460.579, _timer/batch_time=0.330, _timer/data_time=0.020, _timer/model_time=0.310, loss=7.372, lr=0.010, map10=0.108, momentum=0.900, ndcg20=0.061]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid (5/100) loss: 7.362623564612787 | lr: 0.01 | map10: 0.12863219465246267 | map10/std: 0.016430418009910335 | momentum: 0.9 | ndcg20: 0.07577643458120871 | ndcg20/std: 0.007428773133592136\n",
      "* Epoch (5/100) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6/100 * Epoch (train): 100%|| 24/24 [00:26<00:00,  1.10s/it, _timer/_fps=183.210, _timer/batch_time=0.830, _timer/data_time=0.466, _timer/model_time=0.364, loss=7.324, lr=0.010, map10=0.000e+00, momentum=0.900, ndcg20=0.000e+00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (6/100) loss: 7.371885016106611 | lr: 0.01 | map10: 0.0 | map10/std: 0.0 | momentum: 0.9 | ndcg20: 0.0 | ndcg20/std: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6/100 * Epoch (valid): 100%|| 24/24 [00:12<00:00,  1.89it/s, _timer/_fps=455.050, _timer/batch_time=0.334, _timer/data_time=0.021, _timer/model_time=0.313, loss=7.353, lr=0.010, map10=0.129, momentum=0.900, ndcg20=0.069]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid (6/100) loss: 7.398236951764845 | lr: 0.01 | map10: 0.13696408808626087 | map10/std: 0.015109525812340294 | momentum: 0.9 | ndcg20: 0.07902482970068786 | ndcg20/std: 0.007289206484509295\n",
      "* Epoch (6/100) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7/100 * Epoch (train): 100%|| 24/24 [00:26<00:00,  1.09s/it, _timer/_fps=178.209, _timer/batch_time=0.853, _timer/data_time=0.492, _timer/model_time=0.361, loss=7.330, lr=0.010, map10=0.000e+00, momentum=0.900, ndcg20=0.000e+00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (7/100) loss: 7.364537566229207 | lr: 0.01 | map10: 0.0 | map10/std: 0.0 | momentum: 0.9 | ndcg20: 0.0 | ndcg20/std: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7/100 * Epoch (valid): 100%|| 24/24 [00:13<00:00,  1.72it/s, _timer/_fps=349.563, _timer/batch_time=0.435, _timer/data_time=0.023, _timer/model_time=0.411, loss=7.382, lr=0.010, map10=0.119, momentum=0.900, ndcg20=0.071]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid (7/100) loss: 7.377548524086049 | lr: 0.01 | map10: 0.13614802711846813 | map10/std: 0.01717392031770509 | momentum: 0.9 | ndcg20: 0.07831891462305524 | ndcg20/std: 0.008011540049322263\n",
      "* Epoch (7/100) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8/100 * Epoch (train): 100%|| 24/24 [00:27<00:00,  1.16s/it, _timer/_fps=182.632, _timer/batch_time=0.832, _timer/data_time=0.472, _timer/model_time=0.361, loss=7.309, lr=0.010, map10=0.000e+00, momentum=0.900, ndcg20=0.000e+00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (8/100) loss: 7.353062136441666 | lr: 0.01 | map10: 0.0 | map10/std: 0.0 | momentum: 0.9 | ndcg20: 0.0 | ndcg20/std: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8/100 * Epoch (valid): 100%|| 24/24 [00:13<00:00,  1.78it/s, _timer/_fps=403.384, _timer/batch_time=0.377, _timer/data_time=0.024, _timer/model_time=0.352, loss=7.309, lr=0.010, map10=0.130, momentum=0.900, ndcg20=0.071]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid (8/100) loss: 7.3192568778991705 | lr: 0.01 | map10: 0.14380098845785028 | map10/std: 0.015081447412352855 | momentum: 0.9 | ndcg20: 0.08099902511037739 | ndcg20/std: 0.007553835326096513\n",
      "* Epoch (8/100) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9/100 * Epoch (train): 100%|| 24/24 [00:28<00:00,  1.18s/it, _timer/_fps=180.187, _timer/batch_time=0.844, _timer/data_time=0.477, _timer/model_time=0.366, loss=7.297, lr=0.010, map10=0.000e+00, momentum=0.900, ndcg20=0.000e+00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (9/100) loss: 7.35384136983101 | lr: 0.01 | map10: 0.0 | map10/std: 0.0 | momentum: 0.9 | ndcg20: 0.0 | ndcg20/std: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9/100 * Epoch (valid): 100%|| 24/24 [00:13<00:00,  1.79it/s, _timer/_fps=418.940, _timer/batch_time=0.363, _timer/data_time=0.024, _timer/model_time=0.339, loss=7.288, lr=0.010, map10=0.117, momentum=0.900, ndcg20=0.066]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid (9/100) loss: 7.3198788282887035 | lr: 0.01 | map10: 0.13838831229715157 | map10/std: 0.017002189835316494 | momentum: 0.9 | ndcg20: 0.07992180320403434 | ndcg20/std: 0.0076521321793722495\n",
      "* Epoch (9/100) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/100 * Epoch (train): 100%|| 24/24 [00:29<00:00,  1.22s/it, _timer/_fps=140.758, _timer/batch_time=1.080, _timer/data_time=0.673, _timer/model_time=0.407, loss=7.281, lr=0.010, map10=0.000e+00, momentum=0.900, ndcg20=0.000e+00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (10/100) loss: 7.34513983694923 | lr: 0.01 | map10: 0.0 | map10/std: 0.0 | momentum: 0.9 | ndcg20: 0.0 | ndcg20/std: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/100 * Epoch (valid): 100%|| 24/24 [00:13<00:00,  1.75it/s, _timer/_fps=451.729, _timer/batch_time=0.336, _timer/data_time=0.023, _timer/model_time=0.313, loss=7.265, lr=0.010, map10=0.128, momentum=0.900, ndcg20=0.068]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid (10/100) loss: 7.30663888943906 | lr: 0.01 | map10: 0.14827104905583208 | map10/std: 0.014963807276094919 | momentum: 0.9 | ndcg20: 0.08204439229128377 | ndcg20/std: 0.007086015865302367\n",
      "* Epoch (10/100) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/100 * Epoch (train): 100%|| 24/24 [00:28<00:00,  1.19s/it, _timer/_fps=179.093, _timer/batch_time=0.849, _timer/data_time=0.460, _timer/model_time=0.389, loss=7.268, lr=0.010, map10=0.000e+00, momentum=0.900, ndcg20=0.000e+00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (11/100) loss: 7.340847535796513 | lr: 0.01 | map10: 0.0 | map10/std: 0.0 | momentum: 0.9 | ndcg20: 0.0 | ndcg20/std: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/100 * Epoch (valid): 100%|| 24/24 [00:13<00:00,  1.81it/s, _timer/_fps=392.955, _timer/batch_time=0.387, _timer/data_time=0.026, _timer/model_time=0.361, loss=7.261, lr=0.010, map10=0.107, momentum=0.900, ndcg20=0.066]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid (11/100) loss: 7.318968758362018 | lr: 0.01 | map10: 0.13772945119845162 | map10/std: 0.016152898229630758 | momentum: 0.9 | ndcg20: 0.07973823644072806 | ndcg20/std: 0.007371894774589045\n",
      "* Epoch (11/100) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/100 * Epoch (train): 100%|| 24/24 [00:27<00:00,  1.15s/it, _timer/_fps=186.073, _timer/batch_time=0.817, _timer/data_time=0.438, _timer/model_time=0.378, loss=7.264, lr=0.010, map10=0.000e+00, momentum=0.900, ndcg20=0.000e+00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (12/100) loss: 7.338669200922481 | lr: 0.01 | map10: 0.0 | map10/std: 0.0 | momentum: 0.9 | ndcg20: 0.0 | ndcg20/std: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/100 * Epoch (valid): 100%|| 24/24 [00:12<00:00,  1.85it/s, _timer/_fps=457.571, _timer/batch_time=0.332, _timer/data_time=0.022, _timer/model_time=0.310, loss=7.252, lr=0.010, map10=0.123, momentum=0.900, ndcg20=0.070]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid (12/100) loss: 7.317331356402264 | lr: 0.01 | map10: 0.14219064494434572 | map10/std: 0.01487801719848606 | momentum: 0.9 | ndcg20: 0.07935756627494928 | ndcg20/std: 0.006998321477483123\n",
      "* Epoch (12/100) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13/100 * Epoch (train): 100%|| 24/24 [00:26<00:00,  1.11s/it, _timer/_fps=182.742, _timer/batch_time=0.832, _timer/data_time=0.473, _timer/model_time=0.359, loss=7.292, lr=0.010, map10=0.000e+00, momentum=0.900, ndcg20=0.000e+00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (13/100) loss: 7.338112532381979 | lr: 0.01 | map10: 0.0 | map10/std: 0.0 | momentum: 0.9 | ndcg20: 0.0 | ndcg20/std: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13/100 * Epoch (valid): 100%|| 24/24 [00:13<00:00,  1.84it/s, _timer/_fps=419.391, _timer/batch_time=0.362, _timer/data_time=0.022, _timer/model_time=0.340, loss=7.281, lr=0.010, map10=0.119, momentum=0.900, ndcg20=0.073]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid (13/100) loss: 7.309316902918532 | lr: 0.01 | map10: 0.147451419151382 | map10/std: 0.01805267610169488 | momentum: 0.9 | ndcg20: 0.08181477647940844 | ndcg20/std: 0.008235468722713152\n",
      "* Epoch (13/100) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14/100 * Epoch (train): 100%|| 24/24 [00:26<00:00,  1.11s/it, _timer/_fps=177.162, _timer/batch_time=0.858, _timer/data_time=0.494, _timer/model_time=0.364, loss=7.294, lr=0.010, map10=0.000e+00, momentum=0.900, ndcg20=0.000e+00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (14/100) loss: 7.3367607628272875 | lr: 0.01 | map10: 0.0 | map10/std: 0.0 | momentum: 0.9 | ndcg20: 0.0 | ndcg20/std: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14/100 * Epoch (valid): 100%|| 24/24 [00:12<00:00,  1.86it/s, _timer/_fps=436.956, _timer/batch_time=0.348, _timer/data_time=0.021, _timer/model_time=0.327, loss=7.279, lr=0.010, map10=0.120, momentum=0.900, ndcg20=0.067]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid (14/100) loss: 7.3171570784208795 | lr: 0.01 | map10: 0.14290125070028745 | map10/std: 0.016033606415020107 | momentum: 0.9 | ndcg20: 0.07931029861731245 | ndcg20/std: 0.00751089527925184\n",
      "* Epoch (14/100) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15/100 * Epoch (train): 100%|| 24/24 [00:26<00:00,  1.09s/it, _timer/_fps=190.844, _timer/batch_time=0.796, _timer/data_time=0.437, _timer/model_time=0.360, loss=7.321, lr=0.010, map10=0.000e+00, momentum=0.900, ndcg20=0.000e+00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (15/100) loss: 7.354373474626351 | lr: 0.01 | map10: 0.0 | map10/std: 0.0 | momentum: 0.9 | ndcg20: 0.0 | ndcg20/std: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15/100 * Epoch (valid): 100%|| 24/24 [00:12<00:00,  1.90it/s, _timer/_fps=412.768, _timer/batch_time=0.368, _timer/data_time=0.024, _timer/model_time=0.345, loss=7.317, lr=0.010, map10=0.131, momentum=0.900, ndcg20=0.070]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid (15/100) loss: 7.334956389547184 | lr: 0.01 | map10: 0.14315529793303533 | map10/std: 0.017343349805413046 | momentum: 0.9 | ndcg20: 0.08156047558152912 | ndcg20/std: 0.008565876107133798\n",
      "* Epoch (15/100) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = ContextBERT4Rec(n_items=len(item2idx)+1, n_dates=n_dates, mask_ratio=0.2, hidden_size=128)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "lr_scheduler = StepLR(optimizer, step_size=20, gamma=0.1)\n",
    "engine = dl.DeviceEngine('cpu')\n",
    "hparams = {\n",
    "    \"anneal_cap\": 0.2,\n",
    "    \"total_anneal_steps\": 6000,\n",
    "}\n",
    "\n",
    "\n",
    "callbacks = [\n",
    "    dl.NDCGCallback(\"logits\", \"targets\", [20]),\n",
    "    dl.MAPCallback(\"logits\", \"targets\", [10]),\n",
    "    dl.OptimizerCallback(\"loss\", accumulation_steps=1),\n",
    "    dl.EarlyStoppingCallback(\n",
    "        patience=5, loader_key=\"valid\", metric_key=\"map10\", minimize=False\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "runner = RecSysRunner()\n",
    "runner.train(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    engine=engine,\n",
    "    hparams=hparams,\n",
    "    scheduler=lr_scheduler,\n",
    "    loaders=loaders,\n",
    "    num_epochs=100,\n",
    "    verbose=True,\n",
    "    timeit=True,\n",
    "    callbacks=callbacks,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fd40b7a5-44af-44b4-9dec-5366ae71bf11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [00:06, 16.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6040\n"
     ]
    }
   ],
   "source": [
    "test_dataset = MyDataset(ds=joined, num_items=n_items, phase='test',item2idx=item2idx)\n",
    "\n",
    "\n",
    "inference_loader = DataLoader(test_dataset, \n",
    "                              batch_size=joined.shape[0]//100, \n",
    "                              collate_fn=collate_fn_train,)\n",
    "\n",
    "preds = []\n",
    "\n",
    "for prediction in tqdm(runner.predict_loader(loader=inference_loader)):\n",
    "    preds.extend(prediction.detach().cpu().numpy().tolist())\n",
    "    \n",
    "print(len(preds))\n",
    "assert len(preds) == joined.shape[0]\n",
    "\n",
    "joined['preds_contextbert4rec'] = preds\n",
    "joined['recs_contextbert4rec_10'] = joined['preds_contextbert4rec'].apply(lambda x: np.argsort(-np.array(x))[:10])\n",
    "joined['recs_contextbert4rec_10'] = joined['recs_contextbert4rec_10'].apply(lambda x: [idx2item[t-1] for t in x])\n",
    "joined['recs_contextbert4rec_5'] = joined['preds_contextbert4rec'].apply(lambda x: np.argsort(-np.array(x))[:5])\n",
    "joined['recs_contextbert4rec_5'] = joined['recs_contextbert4rec_5'].apply(lambda x: [idx2item[t-1] for t in x])\n",
    "joined.drop(['preds_contextbert4rec'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8aba26b2-ff27-4a30-8e0c-5da919ba9ee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ndcg': 0.14897195618042602, 'recall': 0.031896361990104}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_recommender(joined, model_preds='recs_contextbert4rec_10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fcffbae9-24a4-4706-9d75-351f837e35d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ndcg': 0.0911531299562371, 'recall': 0.01706607886901304}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_recommender(joined, model_preds='recs_contextbert4rec_5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebf2684-1f81-40ee-979c-7cdb1d7c5634",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
